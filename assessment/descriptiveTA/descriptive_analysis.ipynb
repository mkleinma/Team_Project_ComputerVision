{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: to derive information from tweet\n",
    "\n",
    "Descriptive analysis of the text:\n",
    "- using packages like NLTK, SpaCy \n",
    "- possible new features: \n",
    "    - .# of words,  # of chars, avg, length of words, most common words, occurrences of chars (e.g., ‘?’, ‘!’), most common smileys, most common multiple smiley occurrences, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "from preprocessing.preprocessing import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items: tweet_data\n",
      "(5000, 9)\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset(subset_name=\"climatevisions_2019_popular\")\n",
    "df = preprocess_dataset(df, selected_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>img_name</th>\n",
       "      <th>language</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-22T12:38:24.000Z</td>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'82582'</td>\n",
       "      <td>b'3918'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'50280'</td>\n",
       "      <td>the UN released a 740 page report compiled ove...</td>\n",
       "      <td>1153283149360762880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-20T09:28:39.000Z</td>\n",
       "      <td>id_1163744643600637952_2019-08-20.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'69820'</td>\n",
       "      <td>b'2456'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'51781'</td>\n",
       "      <td>The Amazon Rainforest, one of the wettest plac...</td>\n",
       "      <td>1163744643600637952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-28T18:51:22.000Z</td>\n",
       "      <td>id_1122574040936452097_2019-04-28.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'69235'</td>\n",
       "      <td>b'87'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'11051'</td>\n",
       "      <td>just learned about climate change https://t.co...</td>\n",
       "      <td>1122574040936452097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-28T13:10:13.000Z</td>\n",
       "      <td>id_1188805167958974465_2019-10-28.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'65465'</td>\n",
       "      <td>b'70'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'6124'</td>\n",
       "      <td>Climate change caused this. https://t.co/JG2Ly...</td>\n",
       "      <td>1188805167958974465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-19T16:30:00.000Z</td>\n",
       "      <td>id_1108042949449969666_2019-03-19.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'62852'</td>\n",
       "      <td>b'976'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'9145'</td>\n",
       "      <td>#GreenNewDeal haters’ plan to address Climate ...</td>\n",
       "      <td>1108042949449969666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                                img_name language  \\\n",
       "0  2019-07-22T12:38:24.000Z  id_1153283149360762880_2019-07-22.jpg\n",
       "       en   \n",
       "1  2019-08-20T09:28:39.000Z  id_1163744643600637952_2019-08-20.jpg\n",
       "       en   \n",
       "2  2019-04-28T18:51:22.000Z  id_1122574040936452097_2019-04-28.jpg\n",
       "       en   \n",
       "3  2019-10-28T13:10:13.000Z  id_1188805167958974465_2019-10-28.jpg\n",
       "       en   \n",
       "4  2019-03-19T16:30:00.000Z  id_1108042949449969666_2019-03-19.jpg\n",
       "       en   \n",
       "\n",
       "  like_count quote_count referenced_tweets retweet_count  \\\n",
       "0   b'82582'     b'3918'              <NA>      b'50280'   \n",
       "1   b'69820'     b'2456'              <NA>      b'51781'   \n",
       "2   b'69235'       b'87'              <NA>      b'11051'   \n",
       "3   b'65465'       b'70'              <NA>       b'6124'   \n",
       "4   b'62852'      b'976'              <NA>       b'9145'   \n",
       "\n",
       "                                                text             tweet_id  \n",
       "0  the UN released a 740 page report compiled ove...  1153283149360762880  \n",
       "1  The Amazon Rainforest, one of the wettest plac...  1163744643600637952  \n",
       "2  just learned about climate change https://t.co...  1122574040936452097  \n",
       "3  Climate change caused this. https://t.co/JG2Ly...  1188805167958974465  \n",
       "4  #GreenNewDeal haters’ plan to address Climate ...  1108042949449969666  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                  5000\n",
       "unique                                                 5000\n",
       "top       the UN released a 740 page report compiled ove...\n",
       "freq                                                      1\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text = df['text']\n",
    "tweet_text.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text', 'stopwords', 'punctuation', 'banana', 'peanut', 'butter']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(series, remove_stopwords=True, remove_punctuation=True):\n",
    "    # Remove URLs -> all tweet texts contain link to tweet\n",
    "    series = series.apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "    \n",
    "    series = series.apply(lambda text: nltk.word_tokenize(text))\n",
    "    if remove_stopwords:\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(['s', '’', \"''\", '“', \"'s\", '”'])\n",
    "        \n",
    "        series = series.apply(lambda words: [word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "    if remove_punctuation:\n",
    "        # Remove punctuation\n",
    "        series = series.apply(lambda words: [word for word in words if word not in string.punctuation])\n",
    "\n",
    "    return series\n",
    "\n",
    "# Usage example:\n",
    "series = pd.Series([\"This is a sample text with stopwords and punctuation! I do banana & peanut butter) https://t.co/BnAg4x5lGS\",\n",
    "                    \"Another example text with URLs and special characters!\"])\n",
    "preprocessed_series = preprocess_text(series)\n",
    "print(preprocessed_series[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 97894\n",
      "Number of characters: 656430\n",
      "Average length of words: 6.705518213577951\n",
      "Average length of tweet: 19.5788\n",
      "Most common words: [('climate', 3370), ('change', 3125), ('Climate', 765), ('ClimateChange', 635), ('climatechange', 615), ('people', 374), ('Change', 361), ('world', 340), ('today', 312), ('action', 303)]\n",
      "Occurrences of characters: {'?': 603, '!': 1097}\n",
      "Most common smileys: []\n",
      "Most common multiple smiley occurrences: {':)': 0, ':D': 0, ':(': 0, ':P': 0}\n"
     ]
    }
   ],
   "source": [
    "tweet_text_no_punctuation = preprocess_text(tweet_text)\n",
    "tweet_text_punctuation = preprocess_text(tweet_text, remove_punctuation=False)\n",
    "\n",
    "# Calculate the number of words\n",
    "num_words = tweet_text_no_punctuation.apply(len).sum()\n",
    "\n",
    "# Calculate the number of characters\n",
    "num_chars = tweet_text_no_punctuation.apply(lambda text: sum(len(word) for word in text)).sum()\n",
    "\n",
    "# Calculate the average length of words\n",
    "avg_word_length = num_chars / num_words\n",
    "\n",
    "# Calculate the average length of a tweet text\n",
    "avg_text_length = tweet_text_no_punctuation.apply(len).mean()\n",
    "\n",
    "# Calculate the most common words\n",
    "word_freq = FreqDist([word for text in tweet_text_no_punctuation for word in text])\n",
    "most_common_words = word_freq.most_common(10)  # Change the number to get more or fewer common words\n",
    "\n",
    "# Calculate the occurrences of characters\n",
    "char_occurrences = {char: sum(text.count(char) for text in tweet_text_punctuation) for char in ['?', '!']}  # Add more characters as needed\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the most common smileys\n",
    "smileys = [':)', ':D', ':(', ':P']  # Add more smileys as needed\n",
    "smiley_freq = FreqDist([word for text in tweet_text_punctuation for word in text if word in smileys])\n",
    "most_common_smileys = smiley_freq.most_common(5)  # Change the number to get more or fewer common smileys\n",
    "\n",
    "# Calculate the most common multiple smiley occurrences\n",
    "multiple_smiley_occurrences = {smiley: sum(text.count(smiley * 2) for text in tweet_text_punctuation) for smiley in smileys}\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of words:\", num_words)\n",
    "print(\"Number of characters:\", num_chars)\n",
    "print(\"Average length of words:\", avg_word_length)\n",
    "print(\"Average length of tweet:\", avg_text_length)\n",
    "print(\"Most common words:\", most_common_words)\n",
    "print(\"Occurrences of characters:\", char_occurrences)\n",
    "print(\"Most common smileys:\", most_common_smileys)\n",
    "print(\"Most common multiple smiley occurrences:\", multiple_smiley_occurrences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet specific metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods:\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "def extract_emojis(text):\n",
    "  emoji_list = []\n",
    "  emojis_iterator = emoji.analyze(text)\n",
    "  for e in emojis_iterator:\n",
    "    v = e.value\n",
    "    emoji_list.append(v.emoji)\n",
    "    # print(v.emoji)\n",
    "    # print(e.data) --> access data as alias\n",
    "  return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>exclamation_marks</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>no_of_emojis</th>\n",
       "      <th>emojis</th>\n",
       "      <th>no_of_distinct_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>[Me, :, Hi, @, BetoORourke, ,, I, ’, m, ..., B...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[🚪, 🚪, 🚪, 🚪]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>[Canada, is, facing, a, climate, emergency.The...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[✅, ✅]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>[Justin, Trudeau, is, talking, about, climate,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[➡, ➡]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>[We, can, turn, the, tide, on, biodiversity, l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[✅, ✅, 🌳, ✅]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>[Today, ,, Gov, ., Cooper, will, testify, befo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[📽, 📽]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  exclamation_marks  \\\n",
       "1500  [Me, :, Hi, @, BetoORourke, ,, I, ’, m, ..., B...                  3   \n",
       "1859  [Canada, is, facing, a, climate, emergency.The...                  0   \n",
       "2214  [Justin, Trudeau, is, talking, about, climate,...                  0   \n",
       "2464  [We, can, turn, the, tide, on, biodiversity, l...                  0   \n",
       "2878  [Today, ,, Gov, ., Cooper, will, testify, befo...                  0   \n",
       "\n",
       "      question_marks  hashtags  mentions  no_of_emojis        emojis  \\\n",
       "1500               1         3         1             4  [🚪, 🚪, 🚪, 🚪]   \n",
       "1859               0         1         0             2        [✅, ✅]   \n",
       "2214               0         3         0             2        [➡, ➡]   \n",
       "2464               0         1         0             4  [✅, ✅, 🌳, ✅]   \n",
       "2878               0         0         0             2        [📽, 📽]   \n",
       "\n",
       "      no_of_distinct_emojis  \n",
       "1500                      1  \n",
       "1859                      1  \n",
       "2214                      1  \n",
       "2464                      2  \n",
       "2878                      1  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "df_results['text'] = preprocess_text(tweet_text, False, False)\n",
    "\n",
    "# get number of exclamation marks in each tweet\n",
    "df_results[\"exclamation_marks\"] = df_results['text'].apply(lambda text: text.count('!'))\n",
    "\n",
    "# get number of question marks in each tweet\n",
    "df_results[\"question_marks\"] = df_results['text'].apply(lambda text: text.count('?'))\n",
    "\n",
    "# get number of hashtags in each tweet\n",
    "df_results[\"hashtags\"] = df_results['text'].apply(lambda text: text.count('#'))\n",
    "\n",
    "# get number of mentions in each tweet\n",
    "df_results[\"mentions\"] = df_results['text'].apply(lambda text: text.count('@'))\n",
    "\n",
    "df_results[\"no_of_emojis\"] = df_results['text'].apply(lambda text: emoji.emoji_count(text))\n",
    "\n",
    "df_results[\"emojis\"] = df_results['text'].apply(lambda text:  [emoji for sublist in extract_emojis(text) for emoji in sublist]) # extract_emojis(text))\n",
    "\n",
    "df_results[\"no_of_distinct_emojis\"] =  df_results['emojis'].apply(lambda emojis: len(set(emojis)))\n",
    "\n",
    "df_results.head()\n",
    "\n",
    "# just for double checking if sth exists\n",
    "df_results[df_results['no_of_emojis'] !=  df_results['no_of_distinct_emojis']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                     [🚨, Es, wird, Zeit, zu, handeln, !, München, r...\n",
       "exclamation_marks                                                        2\n",
       "question_marks                                                           0\n",
       "hashtags                                                                 4\n",
       "mentions                                                                 0\n",
       "no_of_emojis                                                             2\n",
       "emojis                                                              [🚨, 🚨]\n",
       "no_of_distinct_emojis                                                    1\n",
       "Name: 4996, dtype: object"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.iloc[4996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                     [Slide, for, thermal, print, ➡️🌡️, ., This, is...\n",
       "exclamation_marks                                                        0\n",
       "question_marks                                                           0\n",
       "hashtags                                                                 0\n",
       "mentions                                                                 0\n",
       "no_of_emojis                                                             3\n",
       "emojis                                                           [💃, 🏺, 🌍]\n",
       "no_of_distinct_emojis                                                    3\n",
       "Name: 78, dtype: object"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.iloc[78] # TODO: sometimes emojis are wrongly tokenized and then not recognized as emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests\n",
    "delete later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "\n",
    "def extract_emojis(text):\n",
    "  emoji_list = []\n",
    "  # EMOJIS = emoji.EMOJI_DATA\n",
    "  # data = regex.findall(r'\\X', text)\n",
    "  # flags = regex.findall(u'[\\U0001F1E6-\\U0001F1FF]', text) \n",
    "  # print(flags)\n",
    "  # for word in data:\n",
    "  #   if any(char in EMOJIS for char in word):\n",
    "  #       emoji_list.append(word)\n",
    "  # return emoji_list + flags\n",
    "  # emoji package has difficulties with flags\n",
    "\n",
    "\n",
    "  # emoji_list = [word for word in text.split() if str(word.encode('unicode-escape'))[2] == '\\\\' ]\n",
    "\n",
    "  emojis_iterator = emoji.analyze(text)\n",
    "  for e in emojis_iterator:\n",
    "    v = e.value\n",
    "    emoji_list.append(v.emoji)\n",
    "    #print(v.emoji)\n",
    "    # print(e.data) --> access data as alias\n",
    "  # for e in emoji_list:\n",
    "  #   print(f\"{len(e)}  {e}\")\n",
    "  return emoji_list\n",
    "\n",
    "#  return ''.join(c for c in s if c in EMOJIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_em_string = \"🇦🇺 😂 😂 h d 🤟🏾 q 🤟🏻 👆🏻 🏳️‍🌈 🌎 ➡ 🏺 💃 ➡️🌡️ 🎹  👩🏼‍❤️‍💋‍👩🏽  🫃🏻\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🇦🇺 😂 😂 h d 🤟🏾 q 🤟🏻 👆🏻 🏳️‍🌈 🌎 ➡ 🏺 💃 ➡️🌡️ 🎹  👩🏼‍❤️‍💋‍👩🏽  🫃🏻\n"
     ]
    }
   ],
   "source": [
    "print(test_em_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = extract_emojis(test_em_string)  # tweet_text[4996])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🇦🇺', '😂', '😂', '🤟🏾', '🤟🏻', '👆🏻', '🏳️\\u200d🌈', '🌎', '➡', '🏺', '💃', '➡️', '🌡️', '🎹', '👩🏼\\u200d❤️\\u200d💋\\u200d👩🏽', '\\U0001fac3🏻']\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🇦🇺 😂 😂 h d 🤟🏾 q 🤟🏻 👆🏻 🏳️\\u200d🌈 🌎 ➡ 🏺 💃 ➡️🌡️ 🎹  👩🏼\\u200d❤️\\u200d💋\\u200d👩🏽  \\U0001fac3🏻'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.emojize(test_em_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.emoji_count(test_em_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emoji.distinct_emoji_list(test_em_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'match_start': 0, 'match_end': 1, 'emoji': '😂'},\n",
       " {'match_start': 2, 'match_end': 3, 'emoji': '😂'},\n",
       " {'match_start': 8, 'match_end': 10, 'emoji': '🤟🏾'},\n",
       " {'match_start': 13, 'match_end': 15, 'emoji': '🤟🏻'},\n",
       " {'match_start': 16, 'match_end': 18, 'emoji': '👆🏻'},\n",
       " {'match_start': 19, 'match_end': 21, 'emoji': '🇦🇺'},\n",
       " {'match_start': 22, 'match_end': 26, 'emoji': '🏳️\\u200d🌈'},\n",
       " {'match_start': 27, 'match_end': 28, 'emoji': '🌎'},\n",
       " {'match_start': 29, 'match_end': 30, 'emoji': '➡'},\n",
       " {'match_start': 31, 'match_end': 32, 'emoji': '🏺'},\n",
       " {'match_start': 33, 'match_end': 34, 'emoji': '💃'},\n",
       " {'match_start': 35, 'match_end': 37, 'emoji': '➡️'},\n",
       " {'match_start': 37, 'match_end': 39, 'emoji': '🌡️'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.emoji_list(test_em_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':face_with_tears_of_joy: :face_with_tears_of_joy: h d :love-you_gesture_medium-dark_skin_tone: q :love-you_gesture_light_skin_tone: :backhand_index_pointing_up_light_skin_tone: :Australia: :rainbow_flag: :globe_showing_Americas: :right_arrow: :amphora: :woman_dancing: :right_arrow::thermometer:'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(test_em_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas:\n",
    "- flags as nation bezug\n",
    "- count of emojis used\n",
    "- count of distinct emojis used\n",
    "- add NER\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
