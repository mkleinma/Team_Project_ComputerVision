{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "image_path = \"/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/dataset/id_1079890077575204864_2019-01-01.jpg\"\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "model = timm.create_model('vgg19.tv_in1k', pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deepmoji\n",
    "- torchmoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentiNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model does not work - search different one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, AutoTokenizer\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForImageClassification.from_pretrained(\"Ritvik19/sentinet-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Ritvik19/sentinet-v1\")\n",
    "\n",
    "# Example image URL\n",
    "image_path = \"/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/dataset/id_1079890077575204864_2019-01-01.jpg\"\n",
    "\n",
    "# Preprocess the image\n",
    "image = Image.open(requests.get(image_path, stream=True).raw)\n",
    "inputs = tokenizer(image, return_tensors=\"pt\")\n",
    "\n",
    "# Classify the image\n",
    "outputs = model(**inputs)\n",
    "predicted_class = tokenizer.decode(outputs.logits.argmax())\n",
    "print(\"Predicted sentiment:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduction to VLMs: https://huggingface.co/blog/vision_language_pretraining#supporting-vision-language-models-in-%F0%9F%A4%97-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LENS + T5 \n",
    "https://github.com/ContextualAI/lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> negative</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lens import Lens, LensProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "image_path = \"/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/dataset/id_1079890077575204864_2019-01-01.jpg\"\n",
    "image2_path = \"/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/dataset/id_1079890804267610113_2019-01-01.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "question = \"What is the sentiment of the image? Please classify the image' sentiment into positive, negative and neutral.\"\n",
    "\n",
    "lens = Lens()\n",
    "processor = LensProcessor()\n",
    "with torch.no_grad():\n",
    "    samples = processor([image],[question])\n",
    "    output = lens(samples)\n",
    "    prompts = output[\"prompts\"]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\",truncation_side = 'left',padding = True)\n",
    "LLM_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "# passes image description to llm and asks prompt\n",
    "input_ids = tokenizer(samples[\"prompts\"], return_tensors=\"pt\").input_ids\n",
    "outputs = LLM_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViLT -> visual QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: clear\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# download an input image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "text = \"What is the sentiment of the image? Is the image' sentiment positive, negative or neutral?\"\n",
    "\n",
    "# prepare inputs\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "idx = logits.argmax(-1).item()\n",
    "print(\"Predicted answer:\", model.config.id2label[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "question = \"What is the image's sentiment? Is the image' sentiment positive, negative or neutral?\"\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-capfilt-large\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "question = \"What is the image's sentiment? Is the image' sentiment positive, negative or neutral?\"\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1051: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import FlavaProcessor, FlavaModel\n",
    "\n",
    "model = FlavaModel.from_pretrained(\"facebook/flava-full\")\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "inputs = processor(\n",
    "  text=[\"What is the image's sentiment?\"], images=[image], return_tensors=\"pt\", padding=\"max_length\", max_length=77\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "image_embeddings = outputs.image_embeddings # Batch size X (Number of image patches + 1) x Hidden size => 2 X 197 X 768\n",
    "text_embeddings = outputs.text_embeddings # Batch size X (Text sequence length + 1) X Hidden size => 2 X 77 X 768\n",
    "multimodal_embeddings = outputs.multimodal_embeddings # Batch size X (Number of image patches + Text Sequence Length + 3) X Hidden size => 2 X 275 x 768\n",
    "# Multimodal embeddings can be used for multimodal tasks such as VQA\n",
    "\n",
    "# TODO how can I use multimodal embeddings for sentiment analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llava\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m LlavaNextProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-v1.6-34b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaNextForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllava-hf/llava-v1.6-34b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# prepare image and text prompt, using the appropriate prompt template\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3550\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3544\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3545\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3546\u001b[0m )\n\u001b[1;32m   3548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3550\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3552\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3553\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:306\u001b[0m, in \u001b[0;36mLlavaNextForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: LlavaNextConfig):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_modal_projector \u001b[38;5;241m=\u001b[39m LlavaNextMultiModalProjector(config)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_newline \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(config\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mhidden_size, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:417\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    416\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map\n\u001b[0;32m--> 417\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m resolve_trust_remote_code(\n\u001b[1;32m    419\u001b[0m     trust_remote_code, config\u001b[38;5;241m.\u001b[39m_name_or_path, has_local_code, has_remote_code\n\u001b[1;32m    420\u001b[0m )\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:751\u001b[0m, in \u001b[0;36m_LazyAutoMapping.keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 751\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    752\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_attr_from_module(key, name)\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    755\u001b[0m     ]\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    751\u001b[0m     mapping_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 752\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    755\u001b[0m     ]\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:748\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:692\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1500\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1500\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1510\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1512\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1513\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1514\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-34b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-34b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "model.to(\"cuda:0\")\n",
    "\n",
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "question = \"What is the sentiment of the image? Is the image' sentiment positive, negative or neutral?\"\n",
    "prompt = f\"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n",
    "\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: look into https://huggingface.co/flax-community/clip-vision-bert-vqa-ft-6k (needs download of model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import numpy as  np\n",
    "import os\n",
    "from transformers import CLIPProcessor, BertTokenizerFast\n",
    "from model.flax_clip_vision_bert.modeling_clip_vision_bert import FlaxCLIPVisionBertForSequenceClassification\n",
    "image_path = os.path.join('images/val2014', os.listdir('images/val2014')[0])\n",
    "img = read_image(image_path)\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_outputs = clip_processor(images=img)\n",
    "clip_outputs['pixel_values'][0] = clip_outputs['pixel_values'][0].transpose(1,2,0) # Need to transpose images as model expected channel last images.\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = FlaxCLIPVisionBertForSequenceClassification.from_pretrained('flax-community/clip-vision-bert-vqa-ft-6k')\n",
    "\n",
    "text = \"Are there teddy bears in the image?\"\n",
    "\n",
    "tokens = tokenizer([text], return_tensors=\"np\")\n",
    "pixel_values = np.concatenate([clip_outputs['pixel_values']])\n",
    "outputs = model(pixel_values=pixel_values, **tokens)\n",
    "preds = outputs.logits[0]\n",
    "sorted_indices = np.argsort(preds)[::-1] # Get reverse sorted scores\n",
    "top_5_indices = sorted_indices[:5]\n",
    "top_5_tokens = list(map(model.config.id2label.get,top_5_indices))\n",
    "top_5_scores = preds[top_5_indices]\n",
    "print(dict(zip(top_5_tokens, top_5_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "from processing_image import Preprocess\n",
    "from visualizing_image import SingleImageViz\n",
    "from modeling_frcnn import GeneralizedRCNN\n",
    "from utils import Config\n",
    "import utils\n",
    "from transformers import VisualBertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "URL = \"https://vqa.cloudcv.org/media/test2014/COCO_test2014_000000262567.jpg\"\n",
    "OBJ_URL = \"https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/genome/1600-400-20/objects_vocab.txt\"\n",
    "ATTR_URL = \"https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/genome/1600-400-20/attributes_vocab.txt\"\n",
    "VQA_URL = \"https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt\"\n",
    "\n",
    "objids = utils.get_data(OBJ_URL)\n",
    "attrids = utils.get_data(ATTR_URL)\n",
    "vqa_answers = utils.get_data(VQA_URL)\n",
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "\n",
    "# image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "visualbert_vqa = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n",
    "\n",
    "\n",
    "frcnn_visualizer = SingleImageViz(URL, id2obj=objids, id2attr=attrids)\n",
    "# run frcnn\n",
    "images, sizes, scales_yx = image_preprocess(URL)\n",
    "output_dict = frcnn(\n",
    "    images,\n",
    "    sizes,\n",
    "    scales_yx=scales_yx,\n",
    "    padding=\"max_detections\",\n",
    "    max_detections=frcnn_cfg.max_detections,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "features = output_dict.get(\"roi_features\")\n",
    "\n",
    "\n",
    "test_question = \"What is the image's sentiment? Is the image' sentiment positive, negative or neutral?\"\n",
    "\n",
    "inputs = bert_tokenizer(\n",
    "    test_question,\n",
    "    padding=\"max_length\",\n",
    "    max_length=20,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "output_vqa = visualbert_vqa(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    visual_embeds=features,\n",
    "    visual_attention_mask=torch.ones(features.shape[:-1]),\n",
    "    token_type_ids=inputs.token_type_ids,\n",
    "    output_attentions=False,\n",
    ")\n",
    "# get prediction\n",
    "pred_vqa = output_vqa[\"logits\"].argmax(-1)\n",
    "print(\"Question:\", test_question)\n",
    "print(\"prediction from VisualBert VQA:\", vqa_answers[pred_vqa])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textvqa\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textvqa\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "question = \"Which of these categories 'positive', 'negative' or 'neutral' best describes the image sentiment and feeling?\"\n",
    "\n",
    "input_ids = processor(text=question, add_special_tokens=False).input_ids\n",
    "input_ids = [processor.tokenizer.cls_token_id] + input_ids\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n",
    "print(processor.batch_decode(generated_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I539205/Documents/dev/uni/sem2/TeamProject/Team_Project_ComputerVision/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_API_KEY=\"AIzaSyBYG3RuSaxKE3nHgokSsnK62jDTWFkzIw4\" # TODO replace with your API key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- id_1091125232755200000_2019-02-01: pos as humans are smiling, text on signs not considered\n",
    "    - beim 2. Versuch doch: {\"sentiment\": \"negative\", \"confidence\": 0.8, \"explanation\": \"The image is about climate change and the protesters are holding signs that express negative sentiments such as: 'Time is running out', 'Wake up humans you are endangered too' and 'We are running out of time'. \"} -> unsteady?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test models/gemini-pro-vision and models/gemini-1.0-pro-vision-latest\n",
    "model = genai.GenerativeModel('gemini-1.0-pro-vision-latest')  # gemini-pro-vision\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "image_paths = [\"id_1091125232755200000_2019-02-01\", \"id_1091124153367838720_2019-02-01\"]\n",
    "images = []\n",
    "\n",
    "for i, path in enumerate(image_paths):\n",
    "    img_path = f\"/Users/I539205/Library/CloudStorage/OneDrive-students.uni-mannheim.de/2. Semester/Team Project/dataset/02_February/{path}.jpg\"\n",
    "    img = Image.open(\"/Users/I539205/Library/CloudStorage/OneDrive-students.uni-mannheim.de/2. Semester/Team Project/dataset/11_November/id_1190065111618215945_2019-11-01.jpg\")\n",
    "    images.append(img)\n",
    "\n",
    "prompt = \"\"\"What is the sentiment of the image? \n",
    "Please classify the image' sentiment into positive, negative and neutral and provide \n",
    "a confidence score in [0,1] as well as a short explanation. \n",
    "Structure your answer in the same json format as this example and do not add any additional information: \n",
    "{\"sentiment\": \"XX\", \"confidence\": XX, \"explanation\": \"XX\"},\n",
    "\"\"\"\n",
    "all_responses = {}\n",
    "for image in images:\n",
    "    name = pathlib.Path(image.filename).stem\n",
    "    print(name)\n",
    "    response = model.generate_content([prompt, image], stream=True)\n",
    "    response.resolve()\n",
    "    to_markdown(response.text)\n",
    "    # response.text is text structured as json, so convert to dict\n",
    "    r_dict = json.loads(response.text)\n",
    "    all_responses[name] = r_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "id_1157258552051937280_2019-08-02.jpg\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.8, \"explanation\": \"The image is about the UK birth rate hitting an 80-year low. The article is likely to be seen as negative as it is about a decline in the population.\"}\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.8, \"explanation\": \"The image is about the UK birth rate hitting an 80-year low. The article is likely to be seen as negative as it is about a decline in the population.\"}\n",
      "id_1149660227295309825_2019-07-12.jpg\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.8, \"explanation\": \"The image shows Leonardo DiCaprio, an actor known for his environmental activism, looking concerned with a picture of melting ice in the background. This suggests that the sentiment of the image is negative, as it is about the negative effects of climate change.\"}\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.8, \"explanation\": \"The image shows Leonardo DiCaprio, an actor known for his environmental activism, looking concerned with a picture of melting ice in the background. This suggests that the sentiment of the image is negative, as it is about the negative effects of climate change.\"}\n",
      "id_1158830281689903106_2019-08-06.jpg\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 0.9, \"explanation\": \"The image shows a young girl in a yellow raincoat looking at the camera with a neutral expression. There is no clear indication of her emotional state or the sentiment of the image.\"}\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 0.9, \"explanation\": \"The image shows a young girl in a yellow raincoat looking at the camera with a neutral expression. There is no clear indication of her emotional state or the sentiment of the image.\"}\n",
      "id_1137351131989008386_2019-06-08.jpg\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image is of a man in a suit speaking at a conference. His expression is neutral. It is not possible to determine the sentiment of the image from the content.\"}\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image is of a man in a suit speaking at a conference. His expression is neutral. It is not possible to determine the sentiment of the image from the content.\"}\n",
      "id_1157311548228100096_2019-08-02.jpg\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.7, \"explanation\": \"The image shows the negative effects of climate change, such as rising sea levels, melting glaciers, and extreme weather events. It also shows how these effects are caused by human activities, such as burning fossil fuels and deforestation.\"}\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.7, \"explanation\": \"The image shows the negative effects of climate change, such as rising sea levels, melting glaciers, and extreme weather events. It also shows how these effects are caused by human activities, such as burning fossil fuels and deforestation.\"}\n",
      "id_1161590403755327488_2019-08-14.jpg\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image is a close-up of a walrus. The walrus is looking at the camera with its mouth closed. Its expression is neutral.\"}\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image is a close-up of a walrus. The walrus is looking at the camera with its mouth closed. Its expression is neutral.\"}\n",
      "id_1162127258145837056_2019-08-15.jpg\n",
      "503 The model is overloaded. Please try again later.\n",
      "id_1153803700130836480_2019-07-23.jpg\n",
      " {\"sentiment\": \"positive\", \"confidence\": 0.75, \"explanation\": \"The image shows a large group of people gathered in a hall, listening to a speaker on stage. The people are sitting in rows and appear to be engaged and interested in what the speaker is saying. The overall sentiment of the image is positive.\"}\n",
      " {\"sentiment\": \"positive\", \"confidence\": 0.75, \"explanation\": \"The image shows a large group of people gathered in a hall, listening to a speaker on stage. The people are sitting in rows and appear to be engaged and interested in what the speaker is saying. The overall sentiment of the image is positive.\"}\n",
      "id_1142318827650342912_2019-06-22.jpg\n",
      " {\"sentiment\": \"positive\", \"confidence\": 0.6, \"explanation\": \"The image shows a small bird sitting on a branch of yellow flowers. The bird is looking at the camera with a slightly tilted head, which suggests curiosity or interest. The yellow flowers are a symbol of happiness and joy, and the bird is often associated with freedom and hope. The overall impression of the image is one of peace and tranquility.\"}\n",
      " {\"sentiment\": \"positive\", \"confidence\": 0.6, \"explanation\": \"The image shows a small bird sitting on a branch of yellow flowers. The bird is looking at the camera with a slightly tilted head, which suggests curiosity or interest. The yellow flowers are a symbol of happiness and joy, and the bird is often associated with freedom and hope. The overall impression of the image is one of peace and tranquility.\"}\n",
      "id_1152271842410938368_2019-07-19.jpg\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 0.8, \"explanation\": \"The image depicts a group of women working on the beach. Their facial expressions are not clear, so it is difficult to determine their sentiment. The image does not contain any obvious positive or negative elements.\"}\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 0.8, \"explanation\": \"The image depicts a group of women working on the beach. Their facial expressions are not clear, so it is difficult to determine their sentiment. The image does not contain any obvious positive or negative elements.\"}\n",
      "id_1155528889239855104_2019-07-28.jpg\n",
      " {\"sentiment\": \"positive\", \"confidence\": 0.9, \"explanation\": \"The image shows two girls planting a tree. The girls are smiling and look happy. The image is set in a natural outdoor environment. The overall tone of the image is positive.\"}\n",
      " {\"sentiment\": \"positive\", \"confidence\": 0.9, \"explanation\": \"The image shows two girls planting a tree. The girls are smiling and look happy. The image is set in a natural outdoor environment. The overall tone of the image is positive.\"}\n",
      "id_1164157583990460417_2019-08-21.jpg\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.9, \"explanation\": \"The image shows a comparison of a glacier in 1986 and 2019, with the 2019 image showing a significant reduction in the size of the glacier. The sentiment of the image is negative because it highlights the negative effects of climate change.\"}\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.9, \"explanation\": \"The image shows a comparison of a glacier in 1986 and 2019, with the 2019 image showing a significant reduction in the size of the glacier. The sentiment of the image is negative because it highlights the negative effects of climate change.\"}\n",
      "id_1158816769999540224_2019-08-06.jpg\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.9, \"explanation\": \"The image shows a woman working in a dry, cracked field. The text on the image states that climate-related disasters now account for more than 80% of all major internationally reported disasters. This information is likely to make people feel sad or concerned, which is why the sentiment of the image is negative.\"}\n",
      " {\"sentiment\": \"negative\", \"confidence\": 0.9, \"explanation\": \"The image shows a woman working in a dry, cracked field. The text on the image states that climate-related disasters now account for more than 80% of all major internationally reported disasters. This information is likely to make people feel sad or concerned, which is why the sentiment of the image is negative.\"}\n",
      "id_1161262376038985729_2019-08-13.jpg\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image shows four celebrities. Their facial expressions are neutral.\"}\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image shows four celebrities. Their facial expressions are neutral.\"}\n",
      "id_1145773493482610688_2019-07-01.jpg\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image is a list of states in the US. It does not express any sentiment.\"}\n",
      " {\"sentiment\": \"neutral\", \"confidence\": 1, \"explanation\": \"The image is a list of states in the US. It does not express any sentiment.\"}\n",
      "id_1162063285501472768_2019-08-15.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "#folder_path = f\"/Users/I539205/Library/CloudStorage/OneDrive-SAPSE/Uni Master/Team Project/Dataset_small/{path}\"\n",
    "model = genai.GenerativeModel('gemini-1.0-pro-vision-latest')  # gemini-pro-vision\n",
    "\n",
    "prompt = \"\"\"What is the sentiment of the image? \n",
    "Please classify the image' sentiment into positive, negative and neutral and provide \n",
    "a confidence score in [0,1] as well as a short explanation. \n",
    "Structure your answer in the same json format as this example and do not add any additional information: \n",
    "{\"sentiment\": \"XX\", \"confidence\": XX, \"explanation\": \"XX\"}\n",
    "Please ensure the json format is correct and the explanation is in single quotes.\n",
    "\"\"\"\n",
    "\n",
    "# for each file in folder\n",
    "i = 1\n",
    "all_responses = {}\n",
    "for s in range(5):\n",
    "    if s == 0 or s==1:\n",
    "        print(\"Done\")\n",
    "        continue\n",
    "    folder_path =  f\"/Users/I539205/Library/CloudStorage/OneDrive-SAPSE/Uni Master/Team Project/Dataset_small/{s+1}/\"\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not file.endswith(\".jpg\"):\n",
    "            continue\n",
    "        print(file)\n",
    "        i +=1\n",
    "        img = Image.open(os.path.join(folder_path, file))\n",
    "        # generate content\n",
    "        #response = model.generate_content([prompt, img], stream=True, request_options={\"timeout\": 6000})\n",
    "        try:\n",
    "            response = model.generate_content([prompt, img], stream=True, request_options={\"timeout\": 6000})\n",
    "            response.resolve()\n",
    "            to_markdown(response.text)\n",
    "            text = response.text\n",
    "            print(text)\n",
    "            # replace any \" with ' to make it valid json only for text inside \"explanation\" value\n",
    "            start_index = text.find('\"explanation\": \"') + len('\"explanation\": \"')\n",
    "            end_index = text.find('\"}', start_index)\n",
    "            explanation = text[start_index:end_index].replace('\"', \"'\")\n",
    "            output_string = text[:start_index] + explanation + text[end_index:]\n",
    "\n",
    "            print(output_string)\n",
    "            r_dict = json.loads(output_string)\n",
    "            all_responses[file] = r_dict\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            r_dict = {\"sentiment\": \"prediction failed\", \"confidence\": 0, \"explanation\": \"No sentiment detected.\"}\n",
    "            all_responses[file] = r_dict\n",
    "            continue\n",
    "    save_results_to_json(f\"subset_small_part_{s+1}\", all_responses)\n",
    "    save_results_to_csv(f\"subset_small_part_{s+1}\", all_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try making it async\n",
    "import asyncio\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.0-pro-vision-latest')  # gemini-pro-vision\n",
    "\n",
    "folder_path = \"/Users/I539205/Library/CloudStorage/OneDrive-SAPSE/Uni Master/Team Project/Dataset_small/\"\n",
    "prompt = \"\"\"What is the sentiment of the image? \n",
    "Please classify the image' sentiment into positive, negative and neutral and provide \n",
    "a confidence score in [0,1] as well as a short explanation. \n",
    "Structure your answer in the same json format as this example and do not add any additional information: \n",
    "{\"sentiment\": \"XX\", \"confidence\": XX, \"explanation\": \"XX\"},\n",
    "\"\"\"\n",
    "all_responses = {}\n",
    "\n",
    "async def process_image(file: str, count) -> str:\n",
    "    print(count)\n",
    "    img = Image.open(os.path.join(folder_path, file))\n",
    "    response = await model.generate_content_async([prompt,img], request_options={\"timeout\": 600})\n",
    "    # TODO: error handling\n",
    "    response.resolve()\n",
    "    to_markdown(response.text)\n",
    "    text = response.text\n",
    "    # replace any \" with ' to make it valid json only for text inside \"explanation\" value\n",
    "    start_index = text.find('\"explanation\": \"') + len('\"explanation\": \"')\n",
    "    end_index = text.find('\"}', start_index)\n",
    "    explanation = text[start_index:end_index].replace('\"', \"'\")\n",
    "    output_string = text[:start_index] + explanation + text[end_index:]\n",
    "\n",
    "    print(output_string)\n",
    "    r_dict = json.loads(output_string)\n",
    "    all_responses[file] = r_dict\n",
    "\n",
    "    return response.text\n",
    "\n",
    "jobs = asyncio.gather(*[process_image(file, i) for i, file in enumerate(os.listdir(folder_path))])\n",
    "results = await jobs  # or run_until_complete(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coversions\n",
    "# convert all_responses to json\n",
    "import json\n",
    "def save_results_to_json(month, all_responses):\n",
    "    with open(f'results/responses_{month}.json', 'w') as f:\n",
    "        json.dump(all_responses, f)\n",
    "\n",
    "\n",
    "# convert all_responses to csv\n",
    "import csv\n",
    "def save_results_to_csv(month, all_responses):\n",
    "    with open(f'results/responses_{month}.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['image', 'sentiment', 'confidence', 'explanation'])\n",
    "        for image, dict in all_responses.items():\n",
    "            writer.writerow([image, dict['sentiment'], dict['confidence'], dict['explanation']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options:  VisionEncoderDecoderModel, VisionTextDualEncoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
