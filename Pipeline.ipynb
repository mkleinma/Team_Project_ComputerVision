{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1163744643600637952_2019-08-20.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_1122574040936452097_2019-04-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_1188805167958974465_2019-10-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_1108042949449969666_2019-03-19.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 img_name\n",
       "0  id_1153283149360762880_2019-07-22.jpg\n",
       "\n",
       "1  id_1163744643600637952_2019-08-20.jpg\n",
       "\n",
       "2  id_1122574040936452097_2019-04-28.jpg\n",
       "\n",
       "3  id_1188805167958974465_2019-10-28.jpg\n",
       "\n",
       "4  id_1108042949449969666_2019-03-19.jpg"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "######## Very basic access to the dataset - let's see what we are working with! #######\n",
    "raw_dataset = h5py.File('climatevisions_2019_popular.h5','r+') \n",
    "dataset = raw_dataset['tweet_data']\n",
    "image_directory = 'C:\\\\Users\\\\Admin\\\\Documents\\\\Dataset_small\\\\'\n",
    "cols_to_strip = ['created_at', 'img_name', 'language', 'referenced_tweets', 'text', 'tweet_id']   \n",
    "\n",
    "data_dict = {}\n",
    "# Iterate through the keys (assuming each key is a column name)\n",
    "for key in dataset.keys():\n",
    "     # Access the data for each column\n",
    "     column_data = dataset[key][:]\n",
    "        \n",
    "     # Store the data in the dictionary with the column name as the key\n",
    "     data_dict[key] = column_data\n",
    "     \n",
    "df = pd.DataFrame(data_dict)\n",
    "df[cols_to_strip] = df[cols_to_strip].astype('string')\n",
    "df[cols_to_strip] = df[cols_to_strip].replace(to_replace=r'^b\\':?(.*)\\'$', value=r'\\1', regex=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.dtypes\n",
    "\n",
    "## only keep images here\n",
    "# drop all columns exepct img_ columns\n",
    "selected_columns = ['img_name']\n",
    "df_selected = df.loc[:, selected_columns]\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 57.6ms\n",
      "Speed: 1.0ms preprocess, 57.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 (no detections), 398.9ms\n",
      "Speed: 1.0ms preprocess, 398.9ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 bird, 428.4ms\n",
      "Speed: 3.5ms preprocess, 428.4ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['bird']\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x608 1 person, 449.5ms\n",
      "Speed: 2.5ms preprocess, 449.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 1 person, 428.5ms\n",
      "Speed: 2.0ms preprocess, 428.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 1 person, 453.5ms\n",
      "Speed: 1.0ms preprocess, 453.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 9 persons, 1 book, 450.1ms\n",
      "Speed: 2.0ms preprocess, 450.1ms inference, 3.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "['book', 'person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 1 person, 1 couch, 427.3ms\n",
      "Speed: 1.5ms preprocess, 427.3ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "['person', 'couch']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 (no detections), 446.7ms\n",
      "Speed: 1.5ms preprocess, 446.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 391.7ms\n",
      "Speed: 1.0ms preprocess, 391.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 7 persons, 424.7ms\n",
      "Speed: 2.0ms preprocess, 424.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 432.7ms\n",
      "Speed: 1.5ms preprocess, 432.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 persons, 420.0ms\n",
      "Speed: 2.0ms preprocess, 420.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 person, 404.3ms\n",
      "Speed: 2.0ms preprocess, 404.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 7 persons, 3 airplanes, 8 ties, 445.6ms\n",
      "Speed: 2.0ms preprocess, 445.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "['person', 'tie', 'airplane']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## Write into .csv file\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from openpyxl import Workbook\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "header = [\"image_name\", \"object_detection_results\", \"description_short\", \"description_long\", \"description_comparison\"]\n",
    "moondream_captions = {}\n",
    "aisak_captions = {}\n",
    "\n",
    "ws.append(header)\n",
    "\n",
    "i = 0\n",
    "count = 0\n",
    "for img_name in image_data: \n",
    "    # Load a model\n",
    "    model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "    animals = ['bear', 'penguin', 'polar bear'] \n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    " \n",
    "    image = Image.open(image_directory + img_name)\n",
    "   \n",
    "    results = model(image)  # predict on an image or image directory\n",
    "    names = model.names\n",
    "    #bounding boxes and confidence scores\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "        result.save()\n",
    "        #result.show()\n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "        \n",
    "        if ('person' in detected_objects):\n",
    "            #TODO FACE RECOGNITION\n",
    "            print('person')\n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # long description model\n",
    "    model_id = \"vikhyatk/moondream2\"\n",
    "    revision = \"2024-04-02\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, trust_remote_code=True, revision=revision\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    " \n",
    "    enc_image = model.encode_image(image)\n",
    "    moonDreamResult = model.answer_question(enc_image, \"Describe this image.\", tokenizer)\n",
    "    moondream_captions[img_name] = moonDreamResult\n",
    "    \n",
    "    # short description model\n",
    "    imgToText = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "    aisakResult = imgToText(image)\n",
    "    aisak_captions[img_name] = aisakResult\n",
    "    \n",
    "    \n",
    "    # Depth Anything for Depth Estimation - Foreground/Background\n",
    "    #pipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-large-hf\")\n",
    "    #depth = pipe(image)[\"depth\"]\n",
    "    #cv2.imshow('Depth Anything', depth)\n",
    "    object_string = ''\n",
    "    for index, object in enumerate(detected_objects):\n",
    "        if (index < len(detected_objects)):\n",
    "            object_string += object + ',' + ' '\n",
    "        else:\n",
    "            object_string += object\n",
    "\n",
    "    i+=1\n",
    "    if (i == 15):\n",
    "        break\n",
    "    \n",
    "    mapEntry = aisakResult[0]\n",
    "    firstValueShort = next(iter(mapEntry.values()))\n",
    "\n",
    "    data_row = [img_name, object_string, moonDreamResult, firstValueShort, '']\n",
    "    ws.append(data_row)\n",
    "        \n",
    "    count += 1\n",
    "    detected_objects.clear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.873517910639445\n",
      "Average:  0.8720777829488119\n",
      "Average:  0.903128465016683\n",
      "Average:  0.9182259241739908\n",
      "Average:  0.9083538850148519\n",
      "Average:  0.8902325630187988\n",
      "Average:  0.9210977554321289\n",
      "Average:  0.8911571502685547\n",
      "Average:  0.9032729466756185\n",
      "Average:  0.9049077828725179\n",
      "Average:  0.8510869344075521\n",
      "Average:  0.9078140258789062\n",
      "Average:  0.9079781373341879\n",
      "Average:  0.8865758577982584\n",
      "Average:  0.8991026878356934\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "\n",
    "## Comparison of the three captions ###\n",
    "with open(\"gemini-captions.json\", \"r\") as file:\n",
    "    gemini_captions = json.load(file)\n",
    "\n",
    "i = 0\n",
    "for img_name in image_data:\n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    "    image = Image.open(image_directory + img_name)\n",
    "    gemini_caption = gemini_captions[img_name]\n",
    "    moondream_caption = moondream_captions[img_name]\n",
    "    aisak_caption = aisak_captions[img_name]\n",
    "    aisak_caption = (aisak_caption[0])['generated_text']\n",
    "    #print(gemini_caption)\n",
    "    #print(moondream_caption)\n",
    "    #print(aisak_caption)\n",
    "    \n",
    "    \n",
    "    image.show()\n",
    "    i += 1\n",
    "    sbert = AutoModel.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "    tokens = tokenizer([gemini_caption, moondream_caption, aisak_caption], padding=True, truncation=True, return_tensors='pt')\n",
    "    x = sbert(tokens[\"input_ids\"], tokens[\"attention_mask\"]).pooler_output\n",
    "    similarity_matrix = pairwise.cosine_similarity(x.detach().numpy())\n",
    "    #print(\"Similarities: \" + str(similarity_matrix[0,1]) + \" \" + similarity_matrix[0,2] + \" \" + similarity_matrix[1,2])\n",
    "    avg_sim = (similarity_matrix[0,1] + similarity_matrix[0,2] + similarity_matrix[1,2]) / 3\n",
    "    print(\"Average: \" , avg_sim)\n",
    "    if (i == 15): \n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "### Confidence   ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
