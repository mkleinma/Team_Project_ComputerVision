{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(5000, 9)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "######## Very basic access to the dataset - let's see what we are working with! #######\n",
    "raw_dataset = h5py.File('climatevisions_2019_popular.h5','r+') \n",
    "dataset = raw_dataset['tweet_data']\n",
    "image_directory = 'C:\\\\Users\\\\Admin\\\\Documents\\\\Dataset_small\\\\'\n",
    "cols_to_strip = ['created_at', 'img_name', 'language', 'referenced_tweets', 'text', 'tweet_id']   \n",
    "\n",
    "data_dict = {}\n",
    "# Iterate through the keys (assuming each key is a column name)\n",
    "for key in dataset.keys():\n",
    "     # Access the data for each column\n",
    "     column_data = dataset[key][:]\n",
    "        \n",
    "     # Store the data in the dictionary with the column name as the key\n",
    "     data_dict[key] = column_data\n",
    "     \n",
    "df = pd.DataFrame(data_dict)\n",
    "df[cols_to_strip] = df[cols_to_strip].astype('string')\n",
    "df[cols_to_strip] = df[cols_to_strip].replace(to_replace=r'^b\\':?(.*)\\'$', value=r'\\1', regex=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.dtypes\n",
    "\n",
    "## only keep images here\n",
    "# drop all columns exepct img_ columns\n",
    "selected_columns = ['img_name']\n",
    "df_selected = df.loc[:, selected_columns]\n",
    "df_selected.head()\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 77.7ms\n",
      "Speed: 2.6ms preprocess, 77.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.96043956, 0.8199624, 0.84015167\n",
      "Average:  0.8735178311665853\n",
      "\n",
      "0: 416x640 (no detections), 20.7ms\n",
      "Speed: 1.0ms preprocess, 20.7ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.93199754, 0.83413255, 0.8501033\n",
      "Average:  0.8720777829488119\n",
      "\n",
      "0: 480x640 1 bird, 31.2ms\n",
      "Speed: 3.0ms preprocess, 31.2ms inference, 10.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['bird']\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9622147, 0.8615202, 0.88565075\n",
      "Average:  0.9031285444895426\n",
      "\n",
      "0: 640x608 1 person, 41.1ms\n",
      "Speed: 2.5ms preprocess, 41.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "['person']\n",
      "Predicted label:  disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.94819474, 0.92981994, 0.87666273\n",
      "Average:  0.9182257652282715\n",
      "\n",
      "0: 512x640 1 person, 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "Predicted label:  surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9687861, 0.875968, 0.8803078\n",
      "Average:  0.9083539644877116\n",
      "\n",
      "0: 512x640 1 person, 23.2ms\n",
      "Speed: 2.0ms preprocess, 23.2ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "Predicted label:  disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.95590293, 0.85181236, 0.86298215\n",
      "Average:  0.8902324835459391\n",
      "\n",
      "0: 352x640 9 persons, 1 book, 171.3ms\n",
      "Speed: 1.0ms preprocess, 171.3ms inference, 9.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "['book', 'person']\n",
      "Predicted label:  happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.96384436, 0.9152341, 0.88421476\n",
      "Average:  0.9210977554321289\n",
      "\n",
      "0: 416x640 1 person, 1 couch, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "['person', 'couch']\n",
      "Predicted label:  surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.94020826, 0.85486245, 0.8784007\n",
      "Average:  0.8911571502685547\n",
      "\n",
      "0: 448x640 (no detections), 101.2ms\n",
      "Speed: 2.0ms preprocess, 101.2ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.96661425, 0.8814354, 0.8617691\n",
      "Average:  0.9032729466756185\n",
      "\n",
      "0: 480x640 1 person, 32.6ms\n",
      "Speed: 1.5ms preprocess, 32.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "Predicted label:  disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9404683, 0.8889817, 0.8852731\n",
      "Average:  0.9049077033996582\n",
      "\n",
      "0: 480x640 7 persons, 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "Predicted label:  sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9499887, 0.80549806, 0.7977738\n",
      "Average:  0.8510868549346924\n",
      "\n",
      "0: 640x384 1 person, 22.7ms\n",
      "Speed: 2.0ms preprocess, 22.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "['person']\n",
      "Predicted label:  surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9447664, 0.88645184, 0.89222336\n",
      "Average:  0.9078138669331869\n",
      "\n",
      "0: 640x640 2 persons, 27.1ms\n",
      "Speed: 3.0ms preprocess, 27.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "['person']\n",
      "Predicted label:  disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.92845845, 0.90658784, 0.8888881\n",
      "Average:  0.9079781373341879\n",
      "\n",
      "0: 640x384 1 person, 23.6ms\n",
      "Speed: 1.0ms preprocess, 23.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "['person']\n",
      "Predicted label:  sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9262829, 0.8808224, 0.8526229\n",
      "Average:  0.8865760962168375\n",
      "\n",
      "0: 448x640 7 persons, 3 airplanes, 8 ties, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "['person', 'tie', 'airplane']\n",
      "Predicted label:  neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9667012, 0.86994743, 0.8606601\n",
      "Average:  0.8991029262542725\n",
      "\n",
      "0: 640x544 3 persons, 2 ties, 94.1ms\n",
      "Speed: 2.0ms preprocess, 94.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "['tie', 'person']\n",
      "Predicted label:  anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.95445585, 0.9084374, 0.90367115\n",
      "Average:  0.9221881230672201\n",
      "\n",
      "0: 448x640 7 persons, 1 airplane, 9 ties, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "['person', 'tie', 'airplane']\n",
      "Predicted label:  neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.26992, 0.32475388, 0.86637014\n",
      "Average:  0.4870146910349528\n",
      "\n",
      "0: 448x640 9 persons, 2 skiss, 1 snowboard, 22.6ms\n",
      "Speed: 2.0ms preprocess, 22.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "['person', 'snowboard', 'skis']\n",
      "Predicted label:  disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.89004815, 0.8959971, 0.90223324\n",
      "Average:  0.8960928916931152\n",
      "\n",
      "0: 640x544 5 persons, 1 car, 35.4ms\n",
      "Speed: 3.0ms preprocess, 35.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "['person', 'car']\n",
      "Predicted label:  happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.9665592, 0.9200032, 0.89857966\n",
      "Average:  0.9283806482950846\n",
      "\n",
      "0: 480x640 1 person, 24.5ms\n",
      "Speed: 6.5ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "Predicted label:  surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: 0.93611336, 0.82761157, 0.8636832\n",
      "Average:  0.8758026758829752\n"
     ]
    }
   ],
   "source": [
    "## Write into .csv file\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import pairwise\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from openpyxl import Workbook\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "header = [\"image_name\", \"object_detection_results\", \"aisak_description\", \"moondream_description\", \"gemini_description\",\"facial_emotion\", \"confidence\"]\n",
    "moondream_captions = {}\n",
    "aisak_captions = {}\n",
    "gemini_captions = {}\n",
    "\n",
    "with open(\"gemini-captions.json\", \"r\") as file:\n",
    "    gemini_captions = json.load(file) ### read out captions\n",
    "\n",
    "ws.append(header)\n",
    "\n",
    "# Required lists and co\n",
    "animals = ['bear', 'penguin', 'polar bear'] \n",
    "\n",
    "\n",
    "# All Models so they are only loaded once\n",
    "yolo_model = YOLO(\"yolov8n.pt\").to(device)\n",
    "emotion_processor = AutoImageProcessor.from_pretrained(\"Rajaram1996/FacialEmoRecog\")\n",
    "emotion_model = AutoModelForImageClassification.from_pretrained(\"Rajaram1996/FacialEmoRecog\").to(device)\n",
    "long_desc_model_id = \"vikhyatk/moondream2\"\n",
    "long_desc_revision = \"2024-04-02\"\n",
    "long_desc_model = AutoModelForCausalLM.from_pretrained(\n",
    "    long_desc_model_id, trust_remote_code=True, revision=long_desc_revision\n",
    ").to(device)\n",
    "long_desc_tokenizer = AutoTokenizer.from_pretrained(long_desc_model_id, revision=long_desc_revision)\n",
    "short_desc_pipeline = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "sbert_model = AutoModel.from_pretrained(\"Voicelab/sbert-base-cased-pl\").to(device)\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "\n",
    "i = 0 ### only temporary to test for some images and not all (locally)\n",
    "for img_name in image_data: \n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    "    image = Image.open(image_directory + img_name)\n",
    "    #image.show()\n",
    "    results = yolo_model(image)\n",
    "    names = yolo_model.names\n",
    "    #bounding boxes and confidence scores\n",
    "    emotion = \"---\"\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detected objects if available\n",
    "        \n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "        \n",
    "        if ('person' in detected_objects):\n",
    "            # Preprocess the image using the image processor\n",
    "            inputs = emotion_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get the predicted logits from the model\n",
    "            outputs = emotion_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get the predicted label index\n",
    "            predicted_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "            # Get the predicted label name using the model's config\n",
    "            emotion = emotion_model.config.id2label[predicted_label]\n",
    "\n",
    "            # Print the predicted label\n",
    "            print(\"Predicted label: \", emotion)\n",
    "            \n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # long description model\n",
    "    enc_image = long_desc_model.encode_image(image).to(device)\n",
    "    moonDreamResult = long_desc_model.answer_question(enc_image, \"Describe this image.\", long_desc_tokenizer)\n",
    "    moondream_captions[img_name] = moonDreamResult\n",
    "    \n",
    "    # short description model\n",
    "    aisakResult = short_desc_pipeline(image)\n",
    "    aisak_captions[img_name] = aisakResult\n",
    "    \n",
    "    # gemini description\n",
    "    geminiResult = gemini_captions[img_name]\n",
    "    \n",
    "    object_string = ''\n",
    "    for index, object in enumerate(detected_objects):\n",
    "        if (index <= len(detected_objects)):\n",
    "            object_string += object + ', '\n",
    "        else:\n",
    "            object_string += object\n",
    "\n",
    "    \n",
    "    if object_string.endswith(','):\n",
    "        object_string = object_string[:-1]\n",
    "    \n",
    "    mapEntry = aisakResult[0]\n",
    "    aisakResult = next(iter(mapEntry.values()))\n",
    "    \n",
    "    ### Comparison of the models ###\n",
    "    tokens = sbert_tokenizer([geminiResult, moonDreamResult, aisakResult], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    x = sbert_model(tokens[\"input_ids\"], tokens[\"attention_mask\"]).pooler_output\n",
    "    similarity_matrix = pairwise.cosine_similarity(x.cpu().detach().numpy())\n",
    "    print(\"Similarities: \" + str(similarity_matrix[0,1]) + \", \" + str(similarity_matrix[0,2]) + \", \" + str(similarity_matrix[1,2]))\n",
    "    avg_sim = (similarity_matrix[0,1] + similarity_matrix[0,2] + similarity_matrix[1,2]) / 3\n",
    "    print(\"Average: \" , avg_sim)\n",
    "\n",
    "\n",
    "    data_row = [img_name, object_string, aisakResult, moonDreamResult, geminiResult, emotion, avg_sim]\n",
    "    \n",
    "    \n",
    "    ws.append(data_row)\n",
    "    detected_objects.clear()\n",
    "    i+=1\n",
    "    if (i == 20):\n",
    "        break\n",
    "    \n",
    "wb.save(\"result.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['St. Bernard']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Testing Playground for DINOv2 - Classifier not trained = fail ##\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "\n",
    "# Load the pretrained DINOv2 model\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Transformation to preprocess the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load an example image\n",
    "image_test = \"id_1088293078027374592_2019-01-24.jpg\"\n",
    "input_image = Image.open(image_directory + image_test)\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_batch = input_batch.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the features from the model\n",
    "    features = model(input_batch)\n",
    "\n",
    "# Define a simple classifier on top of the features\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes=1000):  # Assume 1000 classes for illustration\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate and use the classifier\n",
    "num_classes = 1000  # Change this to the number of classes in your dataset\n",
    "classifier = SimpleClassifier(features.size(1), num_classes)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# Predict the object class\n",
    "with torch.no_grad():\n",
    "    class_logits = classifier(features)\n",
    "    class_probabilities = torch.softmax(class_logits, dim=1)\n",
    "    predicted_classes = torch.argmax(class_probabilities, dim=1)\n",
    "\n",
    "# Download the ImageNet class labels\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "response = requests.get(LABELS_URL)\n",
    "class_labels = response.json()\n",
    "\n",
    "# Map predicted class indices to class names\n",
    "predicted_class_names = [class_labels[idx] for idx in predicted_classes]\n",
    "\n",
    "print(predicted_class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatches\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load LLaMA3 model and spaCy tokenizer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "##STEP 1: Load the captions JSON file to extract object labels.\n",
    "import json\n",
    "# Load the JSON file containing filename-caption pairs\n",
    "with open('gemini-captions.json', 'r') as f:\n",
    "    captions_dict = json.load(f)\n",
    "\n",
    "# Filter out empty captions\n",
    "captions = [caption for caption in captions_dict.values() if caption]\n",
    "\n",
    "#Step2: extract objects from captions using LLaMA3 model\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import spacy\n",
    "\n",
    "# Load LLaMA3 model and spaCy tokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load LLaMA3 model and tokenizer\n",
    "model_llama3 = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model_llama3 = model_llama3.to(device='cuda')\n",
    "tokenizer_llama3 = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\n",
    "model_llama3.eval()\n",
    "\n",
    "# Load spaCy model for tokenization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to load first n captions from JSON file\n",
    "def load_first_n_captions(file_path, n=5):\n",
    "    with open(file_path, 'r') as f:\n",
    "        captions_dict = json.load(f)\n",
    "    captions = list(captions_dict.values())[:n]\n",
    "    return captions\n",
    "\n",
    "# Function to extract objects from caption using LLaMA3 model\n",
    "def extract_objects_from_caption(caption):\n",
    "    # Prepare question for the model\n",
    "    question = f'What objects are in the image? Caption: {caption}'\n",
    "    msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "    # Generate response from the model\n",
    "    res = model_llama3.chat(\n",
    "        image=None,  # We don't use image input here\n",
    "        msgs=msgs,\n",
    "        tokenizer=tokenizer_llama3,\n",
    "        sampling=True,\n",
    "        temperature=0.7,\n",
    "        stream=False  # Ensure streaming is off for single response\n",
    "    )\n",
    "\n",
    "    # Extract objects from the generated text\n",
    "    generated_text = next(res)  # Get the generated response\n",
    "    doc = nlp(generated_text)   # Tokenize generated text using spaCy\n",
    "    objects = [token.text.lower() for token in doc if token.pos_ == 'NOUN']\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Main function to process captions and extract objects\n",
    "def main():\n",
    "    file_path = 'subset-captions.json'\n",
    "    captions = load_first_n_captions(file_path, n=5)\n",
    "\n",
    "    object_labels = set()\n",
    "    for caption in captions:\n",
    "        objects = extract_objects_from_caption(caption)\n",
    "        object_labels.update(objects)\n",
    "\n",
    "    # Convert set to list for grounding DINO model input\n",
    "    object_labels = list(object_labels)\n",
    "    object_labels_text = \" \".join(object_labels)  # Convert to space-separated string\n",
    "#step 3: Grounding DINO model for object detection\n",
    "    # Define the model ID for grounding DINO\n",
    "    model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "\n",
    "    # Initialize processor and model for grounding DINO\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model_dino = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "    # Load image from local file path\n",
    "    image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1088293078027374592_2019-01-24.jpg\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Prepare text input with object labels extracted from LLaMA3\n",
    "    text = object_labels_text\n",
    "\n",
    "    # Ensure the model is on the correct device (e.g., GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_dino.to(device)\n",
    "\n",
    "    # Preprocess inputs\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model_dino(**inputs)\n",
    "\n",
    "    # Post-process the outputs to get object detection results\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs[\"input_ids\"],\n",
    "        box_threshold=0.4,\n",
    "        text_threshold=0.3,\n",
    "        target_sizes=[image.size[::-1]]\n",
    "    )\n",
    "\n",
    "    # Print the detected objects and their confidence scores\n",
    "    print(results)\n",
    "\n",
    "    # Display the image with bounding boxes around detected objects\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw bounding boxes and labels\n",
    "    for detection in results:\n",
    "        if \"score\" in detection:\n",
    "            score = detection[\"score\"].item()\n",
    "            box = detection[\"box\"]\n",
    "            class_name = detection[\"class_name\"]\n",
    "\n",
    "            # Convert normalized coordinates to pixels\n",
    "            xmin, ymin, xmax, ymax = box[0] * image.width, box[1] * image.height, box[2] * image.width, box[3] * image.height\n",
    "\n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Display class name and score\n",
    "            ax.text(xmin, ymin, f\"{class_name} {score:.2f}\", bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.1+cu121\n",
      "Torchvision Version: 0.17.1+cpu\n"
     ]
    }
   ],
   "source": [
    "## Testing Playground for DINOv2 - Classifier not trained = fail ##\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "\n",
    "# Load the pretrained DINOv2 model\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Transformation to preprocess the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load an example image\n",
    "image_test = \"id_1088293078027374592_2019-01-24.jpg\"\n",
    "input_image = Image.open(image_directory + image_test)\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_batch = input_batch.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the features from the model\n",
    "    features = model(input_batch)\n",
    "\n",
    "# Define a simple classifier on top of the features\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes=1000):  # Assume 1000 classes for illustration\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate and use the classifier\n",
    "num_classes = 1000  # Change this to the number of classes in your dataset\n",
    "classifier = SimpleClassifier(features.size(1), num_classes)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# Predict the object class\n",
    "with torch.no_grad():\n",
    "    class_logits = classifier(features)\n",
    "    class_probabilities = torch.softmax(class_logits, dim=1)\n",
    "    predicted_classes = torch.argmax(class_probabilities, dim=1)\n",
    "\n",
    "# Download the ImageNet class labels\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "response = requests.get(LABELS_URL)\n",
    "class_labels = response.json()\n",
    "\n",
    "# Map predicted class indices to class names\n",
    "predicted_class_names = [class_labels[idx] for idx in predicted_classes]\n",
    "\n",
    "print(predicted_class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
