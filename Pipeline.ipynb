{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1163744643600637952_2019-08-20.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_1122574040936452097_2019-04-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_1188805167958974465_2019-10-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_1108042949449969666_2019-03-19.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 img_name\n",
       "0  id_1153283149360762880_2019-07-22.jpg\n",
       "\n",
       "1  id_1163744643600637952_2019-08-20.jpg\n",
       "\n",
       "2  id_1122574040936452097_2019-04-28.jpg\n",
       "\n",
       "3  id_1188805167958974465_2019-10-28.jpg\n",
       "\n",
       "4  id_1108042949449969666_2019-03-19.jpg"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "######## Very basic access to the dataset - let's see what we are working with! #######\n",
    "raw_dataset = h5py.File('climatevisions_2019_popular.h5','r+') \n",
    "dataset = raw_dataset['tweet_data']\n",
    "image_directory = 'C:\\\\Users\\\\Admin\\\\Documents\\\\Dataset_small\\\\'\n",
    "cols_to_strip = ['created_at', 'img_name', 'language', 'referenced_tweets', 'text', 'tweet_id']   \n",
    "\n",
    "data_dict = {}\n",
    "# Iterate through the keys (assuming each key is a column name)\n",
    "for key in dataset.keys():\n",
    "     # Access the data for each column\n",
    "     column_data = dataset[key][:]\n",
    "        \n",
    "     # Store the data in the dictionary with the column name as the key\n",
    "     data_dict[key] = column_data\n",
    "     \n",
    "df = pd.DataFrame(data_dict)\n",
    "df[cols_to_strip] = df[cols_to_strip].astype('string')\n",
    "df[cols_to_strip] = df[cols_to_strip].replace(to_replace=r'^b\\':?(.*)\\'$', value=r'\\1', regex=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.dtypes\n",
    "\n",
    "## only keep images here\n",
    "# drop all columns exepct img_ columns\n",
    "selected_columns = ['img_name']\n",
    "df_selected = df.loc[:, selected_columns]\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 89.9ms\n",
      "Speed: 2.0ms preprocess, 89.9ms inference, 1376.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.873517910639445\n",
      "\n",
      "0: 416x640 (no detections), 442.8ms\n",
      "Speed: 2.0ms preprocess, 442.8ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.8720777829488119\n",
      "\n",
      "0: 480x640 1 bird, 453.6ms\n",
      "Speed: 2.5ms preprocess, 453.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['bird']\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.903128465016683\n",
      "\n",
      "0: 640x608 1 person, 548.4ms\n",
      "Speed: 2.0ms preprocess, 548.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9182259241739908\n",
      "\n",
      "0: 512x640 1 person, 574.8ms\n",
      "Speed: 2.5ms preprocess, 574.8ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9083538850148519\n",
      "\n",
      "0: 512x640 1 person, 455.5ms\n",
      "Speed: 2.0ms preprocess, 455.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.8902325630187988\n",
      "\n",
      "0: 352x640 9 persons, 1 book, 569.0ms\n",
      "Speed: 1.0ms preprocess, 569.0ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "['book', 'person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9210977554321289\n",
      "\n",
      "0: 416x640 1 person, 1 couch, 494.6ms\n",
      "Speed: 1.0ms preprocess, 494.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "['person', 'couch']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.8911571502685547\n",
      "\n",
      "0: 448x640 (no detections), 497.8ms\n",
      "Speed: 2.0ms preprocess, 497.8ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9032729466756185\n",
      "\n",
      "0: 480x640 1 person, 520.0ms\n",
      "Speed: 1.0ms preprocess, 520.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Write into .csv file\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from openpyxl import Workbook\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "header = [\"image_name\", \"object_detection_results\", \"aisak_description\", \"moondream_description\", \"gemini_description\" \"confidence\"]\n",
    "moondream_captions = {}\n",
    "aisak_captions = {}\n",
    "gemini_captions = {}\n",
    "\n",
    "with open(\"gemini-captions.json\", \"r\") as file:\n",
    "    gemini_captions = json.load(file) ### read out captions\n",
    "\n",
    "ws.append(header)\n",
    "\n",
    "i = 0 ### only temporary to test for some images and not all (locally)\n",
    "for img_name in image_data: \n",
    "    # Load a model\n",
    "    model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "    animals = ['bear', 'penguin', 'polar bear'] \n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    " \n",
    "    image = Image.open(image_directory + img_name)\n",
    "   \n",
    "    results = model(image)  # predict on an image or image directory\n",
    "    names = model.names\n",
    "    #bounding boxes and confidence scores\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "        result.save()\n",
    "        #result.show()\n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "        \n",
    "        if ('person' in detected_objects):\n",
    "            #TODO FACE RECOGNITION\n",
    "            print('person')\n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # long description model\n",
    "    model_id = \"vikhyatk/moondream2\"\n",
    "    revision = \"2024-04-02\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, trust_remote_code=True, revision=revision\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    " \n",
    "    enc_image = model.encode_image(image)\n",
    "    moonDreamResult = model.answer_question(enc_image, \"Describe this image.\", tokenizer)\n",
    "    moondream_captions[img_name] = moonDreamResult\n",
    "    \n",
    "    # short description model\n",
    "    imgToText = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "    aisakResult = imgToText(image)\n",
    "    aisak_captions[img_name] = aisakResult\n",
    "    \n",
    "    # gemini description\n",
    "    geminiResult = gemini_captions[img_name]\n",
    "    \n",
    "    object_string = ''\n",
    "    for index, object in enumerate(detected_objects):\n",
    "        if (index < len(detected_objects)):\n",
    "            object_string += object + ',' + ' '\n",
    "        else:\n",
    "            object_string += object\n",
    "\n",
    "    i+=1\n",
    "    if (i == 10):\n",
    "        break\n",
    "    \n",
    "    if object_string.endswith(','):\n",
    "        object_string = object_string[:-1]\n",
    "    \n",
    "    mapEntry = aisakResult[0]\n",
    "    aisakResult = next(iter(mapEntry.values()))\n",
    "    \n",
    "    ### Comparison of the models ###\n",
    "    sbert = AutoModel.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "    tokens = tokenizer([geminiResult, moonDreamResult, aisakResult], padding=True, truncation=True, return_tensors='pt')\n",
    "    x = sbert(tokens[\"input_ids\"], tokens[\"attention_mask\"]).pooler_output\n",
    "    similarity_matrix = pairwise.cosine_similarity(x.detach().numpy())\n",
    "    # print(\"Similarities: \" + str(similarity_matrix[0,1]) + \" \" + similarity_matrix[0,2] + \" \" + similarity_matrix[1,2])\n",
    "    avg_sim = (similarity_matrix[0,1] + similarity_matrix[0,2] + similarity_matrix[1,2]) / 3\n",
    "    print(\"Average: \" , avg_sim)\n",
    "\n",
    "\n",
    "    data_row = [img_name, object_string, aisakResult, moonDreamResult, geminiResult, avg_sim]\n",
    "    \n",
    "    \n",
    "    ws.append(data_row)\n",
    "        \n",
    "    detected_objects.clear\n",
    "    \n",
    "    \n",
    "wb.save(\"result.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['St. Bernard']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Testing Playground - Classifier not trained = fail ##\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "\n",
    "# Load the pretrained DINOv2 model\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Transformation to preprocess the input image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load an example image\n",
    "image_test = \"id_1088293078027374592_2019-01-24.jpg\"\n",
    "input_image = Image.open(image_directory + image_test)\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_batch = input_batch.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the features from the model\n",
    "    features = model(input_batch)\n",
    "\n",
    "# Define a simple classifier on top of the features\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes=1000):  # Assume 1000 classes for illustration\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate and use the classifier\n",
    "num_classes = 1000  # Change this to the number of classes in your dataset\n",
    "classifier = SimpleClassifier(features.size(1), num_classes)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# Predict the object class\n",
    "with torch.no_grad():\n",
    "    class_logits = classifier(features)\n",
    "    class_probabilities = torch.softmax(class_logits, dim=1)\n",
    "    predicted_classes = torch.argmax(class_probabilities, dim=1)\n",
    "\n",
    "# Download the ImageNet class labels\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "response = requests.get(LABELS_URL)\n",
    "class_labels = response.json()\n",
    "\n",
    "# Map predicted class indices to class names\n",
    "predicted_class_names = [class_labels[idx] for idx in predicted_classes]\n",
    "\n",
    "print(predicted_class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
