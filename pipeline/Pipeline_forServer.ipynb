{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(5000, 9)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "raw_dataset = h5py.File('climatevisions_2019_popular.h5','r+') \n",
    "dataset = raw_dataset['tweet_data']\n",
    "image_directory = '/home/tp-socialmedia/Dataset_small/'\n",
    "cols_to_strip = ['created_at', 'img_name', 'language', 'referenced_tweets', 'text', 'tweet_id']   \n",
    "\n",
    "data_dict = {}\n",
    "# Iterate through the keys (assuming each key is a column name)\n",
    "for key in dataset.keys():\n",
    "     # Access the data for each column\n",
    "     column_data = dataset[key][:]\n",
    "        \n",
    "     # Store the data in the dictionary with the column name as the key\n",
    "     data_dict[key] = column_data\n",
    "     \n",
    "df = pd.DataFrame(data_dict)\n",
    "df[cols_to_strip] = df[cols_to_strip].astype('string')\n",
    "df[cols_to_strip] = df[cols_to_strip].replace(to_replace=r'^b\\':?(.*)\\'$', value=r'\\1', regex=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.dtypes\n",
    "\n",
    "## only keep images here\n",
    "# drop all columns exepct img_ columns\n",
    "selected_columns = ['img_name']\n",
    "df_selected = df.loc[:, selected_columns]\n",
    "df_selected.head()\n",
    "\n",
    "print(device)\n",
    "\n",
    "def tensor_to_array(tensor):\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    return tensor.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tp-socialmedia/GroundingDINO/weights\\groundingdino_swint_ogc.pth ; exist: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "file \"c:\\home\\tp-socialmedia\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\" does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m sbert_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoicelab/sbert-base-cased-pl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     57\u001b[0m sbert_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoicelab/sbert-base-cased-pl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m dino_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWEIGHTS_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m### only temporary to test for some images and not all (locally)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m image_data: \n",
      "File \u001b[1;32mc:\\users\\admin\\groundingdino\\groundingdino\\util\\inference.py:30\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_config_path, model_checkpoint_path, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_config_path: \u001b[38;5;28mstr\u001b[39m, model_checkpoint_path: \u001b[38;5;28mstr\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mSLConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     args\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(args)\n",
      "File \u001b[1;32mc:\\users\\admin\\groundingdino\\groundingdino\\util\\slconfig.py:185\u001b[0m, in \u001b[0;36mSLConfig.fromfile\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromfile\u001b[39m(filename):\n\u001b[1;32m--> 185\u001b[0m     cfg_dict, cfg_text \u001b[38;5;241m=\u001b[39m \u001b[43mSLConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file2dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SLConfig(cfg_dict, cfg_text\u001b[38;5;241m=\u001b[39mcfg_text, filename\u001b[38;5;241m=\u001b[39mfilename)\n",
      "File \u001b[1;32mc:\\users\\admin\\groundingdino\\groundingdino\\util\\slconfig.py:79\u001b[0m, in \u001b[0;36mSLConfig._file2dict\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_file2dict\u001b[39m(filename):\n\u001b[0;32m     78\u001b[0m     filename \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mabspath(osp\u001b[38;5;241m.\u001b[39mexpanduser(filename))\n\u001b[1;32m---> 79\u001b[0m     \u001b[43mcheck_file_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m temp_config_dir:\n",
      "File \u001b[1;32mc:\\users\\admin\\groundingdino\\groundingdino\\util\\slconfig.py:23\u001b[0m, in \u001b[0;36mcheck_file_exist\u001b[1;34m(filename, msg_tmpl)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_file_exist\u001b[39m(filename, msg_tmpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misfile(filename):\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg_tmpl\u001b[38;5;241m.\u001b[39mformat(filename))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: file \"c:\\home\\tp-socialmedia\\GroundingDINO\\groundingdino\\config\\GroundingDINO_SwinT_OGC.py\" does not exist"
     ]
    }
   ],
   "source": [
    "## Write into .csv file\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import pairwise\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from openpyxl import Workbook\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import supervision as sv\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "header = [\"image_name\", \"object_detection_results\", \"aisak_description\", \"moondream_description\", \"gemini_description\",\"facial_emotion\", \"text_similarity_semantic\", \"text_similarity_semantic_expression\", \"labels\", \"bounding_boxes\", \"bounding_box_confidence\"]\n",
    "ws.append(header)\n",
    "\n",
    "moondream_captions = {}\n",
    "aisak_captions = {}\n",
    "gemini_captions = {}\n",
    "\n",
    "with open(\"gemini-captions.json\", \"r\") as file:\n",
    "    gemini_captions = json.load(file)\n",
    "\n",
    "\n",
    "# Required lists and co\n",
    "animals = ['bear', 'penguin', 'polar bear'] \n",
    "\n",
    "# Prerequisites for GroundingDINO\n",
    "CONFIG_PATH = os.path.join(\"/home/tp-socialmedia/\", \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
    "WEIGHTS_PATH = os.path.join(\"/home/tp-socialmedia/GroundingDINO/\", \"weights\", WEIGHTS_NAME)\n",
    "TEXT_PROMPT = \"cow, glacier, pig, tomato, bread, visualization, bear, tree, person, plant, planet, hurricane, tornado, bird, flower, dog, earth, planet, poster, iceberg, cylcone, fire, water, penguin, seal, dolphin, turtle, bird, rabbit, bat, volcano, cat, car, tv, tiger, seagull, cow, monkey, bag\"\n",
    "BOX_TRESHOLD = 0.37\n",
    "TEXT_TRESHOLD = 0.37\n",
    "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))\n",
    "\n",
    "# All Models so they are only loaded once\n",
    "yolo_model = YOLO(\"yolov8n.pt\").to(device)\n",
    "emotion_processor = AutoImageProcessor.from_pretrained(\"Rajaram1996/FacialEmoRecog\")\n",
    "emotion_model = AutoModelForImageClassification.from_pretrained(\"Rajaram1996/FacialEmoRecog\").to(device)\n",
    "long_desc_model_id = \"vikhyatk/moondream2\"\n",
    "long_desc_revision = \"2024-04-02\"\n",
    "long_desc_model = AutoModelForCausalLM.from_pretrained(\n",
    "    long_desc_model_id, trust_remote_code=True, revision=long_desc_revision\n",
    ").to(device)\n",
    "long_desc_tokenizer = AutoTokenizer.from_pretrained(long_desc_model_id, revision=long_desc_revision)\n",
    "short_desc_pipeline = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "sbert_model = AutoModel.from_pretrained(\"Voicelab/sbert-base-cased-pl\").to(device)\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "dino_model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
    "\n",
    "i = 0 ### only temporary to test for some images and not all (locally)\n",
    "for img_name in image_data: \n",
    "    labels = \"\"\n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    "    image = Image.open(image_directory + img_name)\n",
    "    image_source_dino, image_dino = load_image(image_directory + img_name)\n",
    "    #image.show()\n",
    "    results = yolo_model(image)\n",
    "    names = yolo_model.names\n",
    "    emotion = \"---\"\n",
    "    \n",
    "    boxes, logits, detected_objects = predict(\n",
    "    model=dino_model, \n",
    "    image=image_dino, \n",
    "    caption=TEXT_PROMPT, \n",
    "    box_threshold=BOX_TRESHOLD, \n",
    "    text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    print(boxes)\n",
    "    print(logits)\n",
    "    print(detected_objects)\n",
    "\n",
    "    \n",
    "    #annotated_frame = annotate(image_source=image_source_dino, boxes=boxes, logits=logits, phrases=detected_objects)\n",
    "    #%matplotlib inline  \n",
    "    #sv.plot_image(annotated_frame, (16, 16))\n",
    "\n",
    "\n",
    " \n",
    "    for result in results:\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detected objects if available\n",
    "        person_count = 0\n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name == \"person\"):\n",
    "                person_count += 1\n",
    "        print(\"Person_count: \" + str(person_count))\n",
    "        if person_count >= 4:\n",
    "            labels += \"crowded\"\n",
    "        \n",
    "        if (person_count >= 1):\n",
    "            # Preprocess the image using the image processor\n",
    "            inputs = emotion_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get the predicted logits from the model\n",
    "            outputs = emotion_model(**inputs)\n",
    "            emotion_logits = outputs.logits\n",
    "\n",
    "            # Get the predicted label index\n",
    "            predicted_label = emotion_logits.argmax(dim=-1).item()\n",
    "\n",
    "            # Get the predicted label name using the model's config\n",
    "            emotion = emotion_model.config.id2label[predicted_label]\n",
    "\n",
    "            # Print the predicted label\n",
    "            print(\"Predicted label: \", emotion)\n",
    "                        \n",
    "    \n",
    "    # long description model\n",
    "    enc_image = long_desc_model.encode_image(image).to(device)\n",
    "    moonDreamResult = long_desc_model.answer_question(enc_image, \"Describe this image.\", long_desc_tokenizer)\n",
    "    moondream_captions[img_name] = moonDreamResult\n",
    "    \n",
    "    # short description model\n",
    "    aisakResult = short_desc_pipeline(image)\n",
    "    aisak_captions[img_name] = aisakResult\n",
    "    \n",
    "    # gemini description\n",
    "    geminiResult = gemini_captions[img_name]\n",
    "    \n",
    "    object_string = ''\n",
    "    for index, object in enumerate(detected_objects):\n",
    "        if (index < len(detected_objects)):\n",
    "            object_string += object + ', '\n",
    "        else:\n",
    "            object_string += object\n",
    "\n",
    "    \n",
    "    if object_string.endswith(','):\n",
    "        object_string = object_string[:-1]\n",
    "    \n",
    "    mapEntry = aisakResult[0]\n",
    "    aisakResult = next(iter(mapEntry.values()))\n",
    "    \n",
    "    ### Comparison of the models ###\n",
    "    tokens = sbert_tokenizer([geminiResult, moonDreamResult, aisakResult], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    x = sbert_model(tokens[\"input_ids\"], tokens[\"attention_mask\"]).pooler_output\n",
    "    similarity_matrix = pairwise.cosine_similarity(x.cpu().detach().numpy())\n",
    "    similarities = [similarity_matrix[0, 1], similarity_matrix[0, 2], similarity_matrix[1, 2]]\n",
    "    similarity_strings = \", \".join(str(sim) for sim in similarities)\n",
    "    print(\"Similarities: \" + similarity_strings)\n",
    "    \n",
    "    # Gemini Results are not available for NSFW pictures - that's why this exception to make sure \n",
    "    avg_sim = 0\n",
    "    if not geminiResult:\n",
    "        avg_sim = similarity_matrix[1,2]\n",
    "    else:\n",
    "        avg_sim = (similarity_matrix[0, 1] + similarity_matrix[0, 2] + similarity_matrix[1, 2]) / 3\n",
    "               \n",
    "    print(\"Similarities: \" + str(similarity_matrix[0,1]) + \", \" + str(similarity_matrix[0,2]) + \", \" + str(similarity_matrix[1,2]))\n",
    "    print(\"Average: \" , avg_sim)\n",
    "    \n",
    "    avg_sim_rating = \"\"\n",
    "    if avg_sim < 0.88:\n",
    "        avg_sim_rating = \"low\"\n",
    "    elif avg_sim > 0.92:\n",
    "        avg_sim_rating = \"high\"\n",
    "    else:\n",
    "        avg_sim_rating = \"medium\"\n",
    "    \n",
    "    boxes_array = tensor_to_array(boxes)\n",
    "    logits_array = tensor_to_array(logits.data)\n",
    "    boxes_string = \"\"\n",
    "    logit_string = \"\"\n",
    "    \n",
    "    for index, coordinate in enumerate(boxes_array):\n",
    "        if (index == len(boxes_array) - 1):\n",
    "            boxes_string = boxes_string + str(coordinate)\n",
    "        else:\n",
    "            boxes_string = boxes_string + str(coordinate) + \",\"\n",
    "        \n",
    "    for index, value in enumerate(logits_array):\n",
    "        if index == len(logits_array) - 1:\n",
    "            logit_string += str(value)\n",
    "        else:\n",
    "            logit_string += str(value) + \", \"\n",
    "\n",
    "\n",
    "    data_row = [img_name, object_string, aisakResult, moonDreamResult, geminiResult, emotion, avg_sim, avg_sim_rating, labels, boxes_string, logit_string]\n",
    "    \n",
    "    \n",
    "    ws.append(data_row)\n",
    "    detected_objects.clear()\n",
    "    i+=1\n",
    "    if (i == 15):\n",
    "        break\n",
    "    \n",
    "wb.save(\"result.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
