{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1163744643600637952_2019-08-20.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_1122574040936452097_2019-04-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_1188805167958974465_2019-10-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_1108042949449969666_2019-03-19.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 img_name\n",
       "0  id_1153283149360762880_2019-07-22.jpg\n",
       "\n",
       "1  id_1163744643600637952_2019-08-20.jpg\n",
       "\n",
       "2  id_1122574040936452097_2019-04-28.jpg\n",
       "\n",
       "3  id_1188805167958974465_2019-10-28.jpg\n",
       "\n",
       "4  id_1108042949449969666_2019-03-19.jpg"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "######## Very basic access to the dataset - let's see what we are working with! #######\n",
    "raw_dataset = h5py.File('climatevisions_2019_popular.h5','r+') \n",
    "dataset = raw_dataset['tweet_data']\n",
    "image_directory = '/home/tp-socialmedia/Dataset_small/'\n",
    "cols_to_strip = ['created_at', 'img_name', 'language', 'referenced_tweets', 'text', 'tweet_id']   \n",
    "\n",
    "data_dict = {}\n",
    "# Iterate through the keys (assuming each key is a column name)\n",
    "for key in dataset.keys():\n",
    "     # Access the data for each column\n",
    "     column_data = dataset[key][:]\n",
    "        \n",
    "     # Store the data in the dictionary with the column name as the key\n",
    "     data_dict[key] = column_data\n",
    "     \n",
    "df = pd.DataFrame(data_dict)\n",
    "df[cols_to_strip] = df[cols_to_strip].astype('string')\n",
    "df[cols_to_strip] = df[cols_to_strip].replace(to_replace=r'^b\\':?(.*)\\'$', value=r'\\1', regex=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.dtypes\n",
    "\n",
    "## only keep images here\n",
    "# drop all columns exepct img_ columns\n",
    "selected_columns = ['img_name']\n",
    "df_selected = df.loc[:, selected_columns]\n",
    "df_selected.head()\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 89.9ms\n",
      "Speed: 2.0ms preprocess, 89.9ms inference, 1376.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.873517910639445\n",
      "\n",
      "0: 416x640 (no detections), 442.8ms\n",
      "Speed: 2.0ms preprocess, 442.8ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.8720777829488119\n",
      "\n",
      "0: 480x640 1 bird, 453.6ms\n",
      "Speed: 2.5ms preprocess, 453.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['bird']\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.903128465016683\n",
      "\n",
      "0: 640x608 1 person, 548.4ms\n",
      "Speed: 2.0ms preprocess, 548.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9182259241739908\n",
      "\n",
      "0: 512x640 1 person, 574.8ms\n",
      "Speed: 2.5ms preprocess, 574.8ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9083538850148519\n",
      "\n",
      "0: 512x640 1 person, 455.5ms\n",
      "Speed: 2.0ms preprocess, 455.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.8902325630187988\n",
      "\n",
      "0: 352x640 9 persons, 1 book, 569.0ms\n",
      "Speed: 1.0ms preprocess, 569.0ms inference, 2.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "['book', 'person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9210977554321289\n",
      "\n",
      "0: 416x640 1 person, 1 couch, 494.6ms\n",
      "Speed: 1.0ms preprocess, 494.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "['person', 'couch']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.8911571502685547\n",
      "\n",
      "0: 448x640 (no detections), 497.8ms\n",
      "Speed: 2.0ms preprocess, 497.8ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  0.9032729466756185\n",
      "\n",
      "0: 480x640 1 person, 520.0ms\n",
      "Speed: 1.0ms preprocess, 520.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n",
      "person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Write into .csv file\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import pairwise\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from openpyxl import Workbook\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "header = [\"image_name\", \"object_detection_results\", \"aisak_description\", \"moondream_description\", \"gemini_description\",\"facial_emotion\", \"confidence\"]\n",
    "moondream_captions = {}\n",
    "aisak_captions = {}\n",
    "gemini_captions = {}\n",
    "\n",
    "with open(\"gemini-captions.json\", \"r\") as file:\n",
    "    gemini_captions = json.load(file) ### read out captions\n",
    "\n",
    "ws.append(header)\n",
    "\n",
    "# Required lists and co\n",
    "animals = ['bear', 'penguin', 'polar bear'] \n",
    "\n",
    "\n",
    "# All Models so they are only loaded once\n",
    "yolo_model = YOLO(\"yolov8n.pt\").to(device)\n",
    "emotion_processor = AutoImageProcessor.from_pretrained(\"Rajaram1996/FacialEmoRecog\")\n",
    "emotion_model = AutoModelForImageClassification.from_pretrained(\"Rajaram1996/FacialEmoRecog\").to(device)\n",
    "long_desc_model_id = \"vikhyatk/moondream2\"\n",
    "long_desc_revision = \"2024-04-02\"\n",
    "long_desc_model = AutoModelForCausalLM.from_pretrained(\n",
    "    long_desc_model_id, trust_remote_code=True, revision=long_desc_revision\n",
    ").to(device)\n",
    "long_desc_tokenizer = AutoTokenizer.from_pretrained(long_desc_model_id, revision=long_desc_revision)\n",
    "short_desc_pipeline = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "sbert_model = AutoModel.from_pretrained(\"Voicelab/sbert-base-cased-pl\").to(device)\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "\n",
    "i = 0 ### only temporary to test for some images and not all (locally)\n",
    "for img_name in image_data: \n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    "    image = Image.open(image_directory + img_name)\n",
    "    #image.show()\n",
    "    results = yolo_model(image)\n",
    "    names = yolo_model.names\n",
    "    #bounding boxes and confidence scores\n",
    "    emotion = \"---\"\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detected objects if available\n",
    "        \n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "        \n",
    "        if ('person' in detected_objects):\n",
    "            # Preprocess the image using the image processor\n",
    "            inputs = emotion_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get the predicted logits from the model\n",
    "            outputs = emotion_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get the predicted label index\n",
    "            predicted_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "            # Get the predicted label name using the model's config\n",
    "            emotion = emotion_model.config.id2label[predicted_label]\n",
    "\n",
    "            # Print the predicted label\n",
    "            print(\"Predicted label: \", emotion)\n",
    "            \n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # long description model\n",
    "    enc_image = long_desc_model.encode_image(image).to(device)\n",
    "    moonDreamResult = long_desc_model.answer_question(enc_image, \"Describe this image.\", long_desc_tokenizer)\n",
    "    moondream_captions[img_name] = moonDreamResult\n",
    "    \n",
    "    # short description model\n",
    "    aisakResult = short_desc_pipeline(image)\n",
    "    aisak_captions[img_name] = aisakResult\n",
    "    \n",
    "    # gemini description\n",
    "    geminiResult = gemini_captions[img_name]\n",
    "    \n",
    "    object_string = ''\n",
    "    for index, object in enumerate(detected_objects):\n",
    "        if (index <= len(detected_objects)):\n",
    "            object_string += object + ', '\n",
    "        else:\n",
    "            object_string += object\n",
    "\n",
    "    \n",
    "    if object_string.endswith(','):\n",
    "        object_string = object_string[:-1]\n",
    "    \n",
    "    mapEntry = aisakResult[0]\n",
    "    aisakResult = next(iter(mapEntry.values()))\n",
    "    \n",
    "    ### Comparison of the models ###\n",
    "    tokens = sbert_tokenizer([geminiResult, moonDreamResult, aisakResult], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    x = sbert_model(tokens[\"input_ids\"], tokens[\"attention_mask\"]).pooler_output\n",
    "    similarity_matrix = pairwise.cosine_similarity(x.cpu().detach().numpy())\n",
    "    print(\"Similarities: \" + str(similarity_matrix[0,1]) + \", \" + str(similarity_matrix[0,2]) + \", \" + str(similarity_matrix[1,2]))\n",
    "    avg_sim = (similarity_matrix[0,1] + similarity_matrix[0,2] + similarity_matrix[1,2]) / 3\n",
    "    print(\"Average: \" , avg_sim)\n",
    "\n",
    "\n",
    "    data_row = [img_name, object_string, aisakResult, moonDreamResult, geminiResult, emotion, avg_sim]\n",
    "    \n",
    "    \n",
    "    ws.append(data_row)\n",
    "    detected_objects.clear()\n",
    "    i+=1\n",
    "    if (i == 6):\n",
    "        break\n",
    "    \n",
    "wb.save(\"result.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
