{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f54474f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name _objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m    \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\h5py\\__init__.py:56\u001b[0m\n\u001b[0;32m     51\u001b[0m _register_lzf()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# --- Public API --------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z, h5pl\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filters\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_hdf5, HLObject, Empty\n",
      "File \u001b[1;32mh5py\\h5f.pyx:29\u001b[0m, in \u001b[0;36minit h5py.h5f\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name _objects"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "######## Very basic access to the dataset - let's see what we are working with! #######\n",
    "raw_dataset = h5py.File('climatevisions_2019_popular.h5','r+') \n",
    "image_directory =  \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\"\n",
    "\n",
    "contents = os.listdir(image_directory)\n",
    "print(contents)\n",
    "\n",
    "# Access the 'upper' data - we only have tweet data \n",
    "for item in raw_dataset.keys():\n",
    "   print(\"Items: \" + item)    \n",
    "    \n",
    "# Access the actual subgroups with data for us - different info we can look at - mostly things provided in Excel by Katharina\n",
    "for item in raw_dataset.require_group('tweet_data').keys():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4313dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>img_name</th>\n",
       "      <th>language</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'2019-07-22T12:38:24.000Z'</td>\n",
       "      <td>b'id_1153283149360762880_2019-07-22.jpg\\n'</td>\n",
       "      <td>b'en'</td>\n",
       "      <td>b'82582'</td>\n",
       "      <td>b'3918'</td>\n",
       "      <td>b'&lt;NA&gt;'</td>\n",
       "      <td>b'50280'</td>\n",
       "      <td>b'the UN released a 740 page report compiled o...</td>\n",
       "      <td>b'1153283149360762880'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'2019-08-20T09:28:39.000Z'</td>\n",
       "      <td>b'id_1163744643600637952_2019-08-20.jpg\\n'</td>\n",
       "      <td>b'en'</td>\n",
       "      <td>b'69820'</td>\n",
       "      <td>b'2456'</td>\n",
       "      <td>b'&lt;NA&gt;'</td>\n",
       "      <td>b'51781'</td>\n",
       "      <td>b'The Amazon Rainforest, one of the wettest pl...</td>\n",
       "      <td>b'1163744643600637952'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'2019-04-28T18:51:22.000Z'</td>\n",
       "      <td>b'id_1122574040936452097_2019-04-28.jpg\\n'</td>\n",
       "      <td>b'en'</td>\n",
       "      <td>b'69235'</td>\n",
       "      <td>b'87'</td>\n",
       "      <td>b'&lt;NA&gt;'</td>\n",
       "      <td>b'11051'</td>\n",
       "      <td>b'just learned about climate change https://t....</td>\n",
       "      <td>b'1122574040936452097'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'2019-10-28T13:10:13.000Z'</td>\n",
       "      <td>b'id_1188805167958974465_2019-10-28.jpg\\n'</td>\n",
       "      <td>b'en'</td>\n",
       "      <td>b'65465'</td>\n",
       "      <td>b'70'</td>\n",
       "      <td>b'&lt;NA&gt;'</td>\n",
       "      <td>b'6124'</td>\n",
       "      <td>b'Climate change caused this. https://t.co/JG2...</td>\n",
       "      <td>b'1188805167958974465'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'2019-03-19T16:30:00.000Z'</td>\n",
       "      <td>b'id_1108042949449969666_2019-03-19.jpg\\n'</td>\n",
       "      <td>b'en'</td>\n",
       "      <td>b'62852'</td>\n",
       "      <td>b'976'</td>\n",
       "      <td>b'&lt;NA&gt;'</td>\n",
       "      <td>b'9145'</td>\n",
       "      <td>b'#GreenNewDeal haters\\xe2\\x80\\x99 plan to add...</td>\n",
       "      <td>b'1108042949449969666'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at                                    img_name  \\\n",
       "0  b'2019-07-22T12:38:24.000Z'  b'id_1153283149360762880_2019-07-22.jpg\\n'   \n",
       "1  b'2019-08-20T09:28:39.000Z'  b'id_1163744643600637952_2019-08-20.jpg\\n'   \n",
       "2  b'2019-04-28T18:51:22.000Z'  b'id_1122574040936452097_2019-04-28.jpg\\n'   \n",
       "3  b'2019-10-28T13:10:13.000Z'  b'id_1188805167958974465_2019-10-28.jpg\\n'   \n",
       "4  b'2019-03-19T16:30:00.000Z'  b'id_1108042949449969666_2019-03-19.jpg\\n'   \n",
       "\n",
       "  language like_count quote_count referenced_tweets retweet_count  \\\n",
       "0    b'en'   b'82582'     b'3918'           b'<NA>'      b'50280'   \n",
       "1    b'en'   b'69820'     b'2456'           b'<NA>'      b'51781'   \n",
       "2    b'en'   b'69235'       b'87'           b'<NA>'      b'11051'   \n",
       "3    b'en'   b'65465'       b'70'           b'<NA>'       b'6124'   \n",
       "4    b'en'   b'62852'      b'976'           b'<NA>'       b'9145'   \n",
       "\n",
       "                                                text                tweet_id  \n",
       "0  b'the UN released a 740 page report compiled o...  b'1153283149360762880'  \n",
       "1  b'The Amazon Rainforest, one of the wettest pl...  b'1163744643600637952'  \n",
       "2  b'just learned about climate change https://t....  b'1122574040936452097'  \n",
       "3  b'Climate change caused this. https://t.co/JG2...  b'1188805167958974465'  \n",
       "4  b'#GreenNewDeal haters\\xe2\\x80\\x99 plan to add...  b'1108042949449969666'  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dataset within the group\n",
    "dataset = raw_dataset['tweet_data']  ## excludes unnecessary information - only tweet_data\n",
    " \n",
    "# Create a dictionary to store column data\n",
    "data_dict = {}\n",
    "    \n",
    "# Iterate through the keys (assuming each key is a column name)\n",
    "for key in dataset.keys():\n",
    "     # Access the data for each column\n",
    "     column_data = dataset[key][:]\n",
    "        \n",
    "     # Store the data in the dictionary with the column name as the key\n",
    "     data_dict[key] = column_data\n",
    "# convert the dictionary to a pandas dataframe\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ace8ec",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1efa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_1163744643600637952_2019-08-20.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_1122574040936452097_2019-04-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_1188805167958974465_2019-10-28.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_1108042949449969666_2019-03-19.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 img_name\n",
       "0  id_1153283149360762880_2019-07-22.jpg\n",
       "\n",
       "1  id_1163744643600637952_2019-08-20.jpg\n",
       "\n",
       "2  id_1122574040936452097_2019-04-28.jpg\n",
       "\n",
       "3  id_1188805167958974465_2019-10-28.jpg\n",
       "\n",
       "4  id_1108042949449969666_2019-03-19.jpg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip of \"b'\" of all strings\n",
    "\n",
    "cols_to_strip = ['created_at', 'img_name', 'language', 'referenced_tweets', 'text', 'tweet_id']   \n",
    "\n",
    "df[cols_to_strip] = df[cols_to_strip].astype('string')\n",
    "df[cols_to_strip] = df[cols_to_strip].replace(to_replace=r'^b\\':?(.*)\\'$', value=r'\\1', regex=True)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.dtypes\n",
    "\n",
    "## only keep images here\n",
    "# drop all columns exepct img_ columns\n",
    "selected_columns = ['img_name']\n",
    "df_selected = df.loc[:, selected_columns]\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e304bc9",
   "metadata": {},
   "source": [
    "print(df_selected['img_name'].iloc[0])\n",
    "\n",
    "Image.open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f07bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>img_name</th>\n",
       "      <th>language</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-22T12:38:24.000Z</td>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'82582'</td>\n",
       "      <td>b'3918'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'50280'</td>\n",
       "      <td>the UN released a 740 page report compiled ove...</td>\n",
       "      <td>1153283149360762880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-20T09:28:39.000Z</td>\n",
       "      <td>id_1163744643600637952_2019-08-20.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'69820'</td>\n",
       "      <td>b'2456'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'51781'</td>\n",
       "      <td>The Amazon Rainforest, one of the wettest plac...</td>\n",
       "      <td>1163744643600637952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-28T18:51:22.000Z</td>\n",
       "      <td>id_1122574040936452097_2019-04-28.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'69235'</td>\n",
       "      <td>b'87'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'11051'</td>\n",
       "      <td>just learned about climate change https://t.co...</td>\n",
       "      <td>1122574040936452097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-28T13:10:13.000Z</td>\n",
       "      <td>id_1188805167958974465_2019-10-28.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'65465'</td>\n",
       "      <td>b'70'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'6124'</td>\n",
       "      <td>Climate change caused this. https://t.co/JG2Ly...</td>\n",
       "      <td>1188805167958974465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-19T16:30:00.000Z</td>\n",
       "      <td>id_1108042949449969666_2019-03-19.jpg</td>\n",
       "      <td>en</td>\n",
       "      <td>b'62852'</td>\n",
       "      <td>b'976'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'9145'</td>\n",
       "      <td>#GreenNewDeal haters’ plan to address Climate ...</td>\n",
       "      <td>1108042949449969666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                                img_name language  \\\n",
       "0  2019-07-22T12:38:24.000Z  id_1153283149360762880_2019-07-22.jpg\n",
       "       en   \n",
       "1  2019-08-20T09:28:39.000Z  id_1163744643600637952_2019-08-20.jpg\n",
       "       en   \n",
       "2  2019-04-28T18:51:22.000Z  id_1122574040936452097_2019-04-28.jpg\n",
       "       en   \n",
       "3  2019-10-28T13:10:13.000Z  id_1188805167958974465_2019-10-28.jpg\n",
       "       en   \n",
       "4  2019-03-19T16:30:00.000Z  id_1108042949449969666_2019-03-19.jpg\n",
       "       en   \n",
       "\n",
       "  like_count quote_count referenced_tweets retweet_count  \\\n",
       "0   b'82582'     b'3918'              <NA>      b'50280'   \n",
       "1   b'69820'     b'2456'              <NA>      b'51781'   \n",
       "2   b'69235'       b'87'              <NA>      b'11051'   \n",
       "3   b'65465'       b'70'              <NA>       b'6124'   \n",
       "4   b'62852'      b'976'              <NA>       b'9145'   \n",
       "\n",
       "                                                text             tweet_id  \n",
       "0  the UN released a 740 page report compiled ove...  1153283149360762880  \n",
       "1  The Amazon Rainforest, one of the wettest plac...  1163744643600637952  \n",
       "2  just learned about climate change https://t.co...  1122574040936452097  \n",
       "3  Climate change caused this. https://t.co/JG2Ly...  1188805167958974465  \n",
       "4  #GreenNewDeal haters’ plan to address Climate ...  1108042949449969666  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1d3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>img_name</th>\n",
       "      <th>language</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4999</td>\n",
       "      <td>5000</td>\n",
       "      <td>28</td>\n",
       "      <td>1237</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>735</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2019-02-07T19:34:34.000Z</td>\n",
       "      <td>id_1153283149360762880_2019-07-22.jpg\\n</td>\n",
       "      <td>en</td>\n",
       "      <td>b'161'</td>\n",
       "      <td>b'2'</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>b'50'</td>\n",
       "      <td>the UN released a 740 page report compiled ove...</td>\n",
       "      <td>1153283149360762880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4823</td>\n",
       "      <td>39</td>\n",
       "      <td>363</td>\n",
       "      <td>5000</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      created_at                                 img_name  \\\n",
       "count                       5000                                     5000   \n",
       "unique                      4999                                     5000   \n",
       "top     2019-02-07T19:34:34.000Z  id_1153283149360762880_2019-07-22.jpg\\n   \n",
       "freq                           2                                        1   \n",
       "\n",
       "       language like_count quote_count referenced_tweets retweet_count  \\\n",
       "count      5000       5000        5000              5000          5000   \n",
       "unique       28       1237         232                 1           735   \n",
       "top          en     b'161'        b'2'              <NA>         b'50'   \n",
       "freq       4823         39         363              5000            51   \n",
       "\n",
       "                                                     text             tweet_id  \n",
       "count                                                5000                 5000  \n",
       "unique                                               5000                 5000  \n",
       "top     the UN released a 740 page report compiled ove...  1153283149360762880  \n",
       "freq                                                    1                    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75373701",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "#### Observations:\n",
    "- entities_cashtags, in_reply_to_user_id, referenced_tweets only missing values\n",
    "- 100-400 missing values: geo_coord_data, geo_coord_type, withheld_copyright, withheld_countrycode\n",
    "- around 300.000 missing: entities_annotations, entities_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0822b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at           0\n",
       "img_name             0\n",
       "language             0\n",
       "like_count           0\n",
       "quote_count          0\n",
       "referenced_tweets    0\n",
       "retweet_count        0\n",
       "text                 0\n",
       "tweet_id             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace string NA to \"real\" missing value for further analysis\n",
    "df = df.replace(r'^NA$', np.nan, regex=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f1dfc",
   "metadata": {},
   "source": [
    "## Analysis of 5000 most popular images (trying to understand patterns by looking at them by hand)\n",
    "- some popular figures such as Trump, Greta Thunberg are visible\n",
    "- a lot of text is presented in form of statistics, pictures of other tweets, text of e.g. political announcements\n",
    "  => Reading out text from images might end up being very important\n",
    "- A lot of emotions displayed (destroyed wall, happy or sad people on images)\n",
    "- Animals exist often in image (e.g. polar bears)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e2479",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing of Models \n",
    "\n",
    "### Goals:\n",
    "- try to find fitting models for our project\n",
    "- produce good results for analysis, try to visualize it to later display in GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d5722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "## Test Model for semantic segmentation - has a few different weights and can e.g. create a mask of a human being but... surely there are better models\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "#img = read_image(image_directory + 'id_1092304223591587840_2019-02-04.jpg')\n",
    "img= read_image(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1091343104781811713_2019-02-01.jpg\")\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "prediction = model(batch)[\"out\"]\n",
    "normalized_masks = prediction.softmax(dim=1)\n",
    "class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "mask = normalized_masks[0, class_to_idx[\"person\"]]\n",
    "\n",
    "print(weights.meta[\"categories\"])\n",
    "to_pil_image(mask).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### describes the image as text - works quite well on several different images we tried so far\n",
    "### partially detects celebreties (for example donald trump) - does not detect greta thunberg but always refers to her as 'girl with braids', does read text from images (OCR), provides a short description of what the image entails\n",
    "### can be used for sentiment analysis\n",
    "from transformers import pipeline\n",
    "imgToText = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "img = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1080547022317412352_2019-01-02.jpg\"\n",
    "result = imgToText(img)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a778dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model that describes scenes very well and gives it some semantic meaning like describing a protest as a protest\n",
    "## long run-time in comparison to the other models\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    " \n",
    "model_id = \"vikhyatk/moondream2\"\n",
    "revision = \"2024-04-02\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, revision=revision\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    " \n",
    "image = Image.open(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1117736292798824448_2019-04-15.jpg\")\n",
    "enc_image = model.encode_image(image)\n",
    "print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a159435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected car with confidence 0.996 at location [818.3, 380.98, 952.25, 472.64]\n",
      "Detected car with confidence 0.944 at location [749.63, 322.39, 806.12, 377.34]\n",
      "Detected car with confidence 0.994 at location [732.65, 436.42, 864.76, 529.21]\n",
      "Detected car with confidence 0.997 at location [909.46, 465.23, 999.8, 528.93]\n",
      "Detected car with confidence 0.983 at location [607.13, 278.32, 664.46, 315.77]\n",
      "Detected traffic light with confidence 0.973 at location [524.26, 142.08, 539.97, 173.87]\n",
      "Detected car with confidence 0.918 at location [771.3, 309.26, 843.28, 354.31]\n",
      "Detected car with confidence 0.993 at location [783.8, 347.87, 867.77, 415.28]\n",
      "Detected car with confidence 0.925 at location [666.3, 223.38, 704.56, 250.42]\n",
      "Detected person with confidence 0.9 at location [347.34, 432.76, 385.72, 524.72]\n",
      "Detected traffic light with confidence 0.975 at location [436.25, 134.66, 450.51, 160.96]\n",
      "Detected bus with confidence 0.958 at location [611.62, 139.43, 787.89, 218.92]\n",
      "Detected truck with confidence 0.984 at location [289.91, 163.15, 379.05, 300.86]\n",
      "Detected traffic light with confidence 0.937 at location [729.69, 146.87, 746.36, 178.67]\n",
      "Detected traffic light with confidence 0.946 at location [527.61, 142.65, 543.82, 174.23]\n",
      "Detected car with confidence 0.937 at location [791.53, 348.89, 874.59, 411.57]\n",
      "Detected car with confidence 0.912 at location [591.8, 240.55, 658.86, 282.93]\n",
      "Detected person with confidence 0.937 at location [614.19, 427.02, 649.95, 518.47]\n",
      "Detected person with confidence 0.955 at location [562.41, 445.41, 605.33, 524.73]\n",
      "Detected car with confidence 0.978 at location [657.88, 337.97, 780.36, 442.6]\n"
     ]
    }
   ],
   "source": [
    "#DETR-ResNet-50\n",
    "#recommended by prof\n",
    "#works well for object detection, detects trucks, people, animals, objects like books, cellphones etc.\n",
    "#does not detect trees, underwater objects... in this case it gives no results\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1082623163039653889_2019-01-08.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "            f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cd38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows the most important objects in the image and removes the background\n",
    "#works well for animals, cars, people.\n",
    "from transformers import pipeline\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1081550972520984576_2019-01-05.jpg\"\n",
    "pipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)\n",
    "pillow_mask = pipe(image_path, return_mask = True) # outputs a pillow mask\n",
    "pillow_image = pipe(image_path) # applies mask on input and returns a pillow image\n",
    "#show the image\n",
    "pillow_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3beb7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\User\\OneDrive\\Desktop\\ProjectCode\\Team_Project_ComputerVision-1\\Dataset_small\\id_1083767274618716160_2019-01-11.jpg: 512x640 1 person, 1 bench, 148.3ms\n",
      "Speed: 9.0ms preprocess, 148.3ms inference, 2601.1ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    }
   ],
   "source": [
    "#works well with humans or animals,\n",
    "#doesnt work well with abstract images: it predicted a floating globe as a doghnut, plus its not giving the confidence\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Assuming you've loaded a pretrained model\n",
    "\n",
    "# Load your image\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1083767274618716160_2019-01-11.jpg\"\n",
    "\n",
    "# Perform object detection on the image\n",
    "results = model(image_path)\n",
    "\n",
    "# Iterate over each detection result and display it\n",
    "for result in results:\n",
    "    # Display the detected objects\n",
    "    result.show()\n",
    "\n",
    "    # Alternatively, you can save the annotated image\n",
    "    result.save()\n",
    "\n",
    "    # Accessing the detections\n",
    "    detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "\n",
    "    if detections is not None:\n",
    "        # Extracting bounding boxes, classes, and confidence scores\n",
    "        boxes = detections[:, :4]  # Extract bounding boxes\n",
    "        confidences = detections[:, 4]  # Extract confidence scores\n",
    "        class_ids = detections[:, 5]  # Extract class IDs\n",
    "\n",
    "        # Print the detected objects\n",
    "        for box, confidence, class_id in zip(boxes, confidences, class_ids):\n",
    "            print(f\"Detected class {class_id.item()} with confidence {confidence.item()} at location {box.tolist()}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0445d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'normal', 'score': 0.9998407363891602},\n",
       " {'label': 'nsfw', 'score': 0.00015927021740935743}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal or not-safe-for-work image classification\n",
    "# Use a pipeline as a high-level helper\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "\n",
    "img = Image.open(\"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1093803054376325121_2019-02-08.jpg\")\n",
    "classifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\n",
    "classifier(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9158293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 96.5ms\n",
      "Speed: 3.5ms preprocess, 96.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "[]\n",
      "\n",
      "0: 416x640 (no detections), 105.8ms\n",
      "Speed: 3.0ms preprocess, 105.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "[]\n",
      "\n",
      "0: 480x640 1 bird, 107.8ms\n",
      "Speed: 5.0ms preprocess, 107.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['bird']\n",
      "Image id_1188805167958974465_2019-10-28.jpg not found. Skipping...\n",
      "Image id_1108042949449969666_2019-03-19.jpg not found. Skipping...\n",
      "Image id_1117808992972148743_2019-04-15.jpg not found. Skipping...\n",
      "\n",
      "0: 352x640 9 persons, 1 book, 101.0ms\n",
      "Speed: 3.2ms preprocess, 101.0ms inference, 3.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "['book', 'person']\n",
      "\n",
      "0: 416x640 1 person, 1 couch, 92.2ms\n",
      "Speed: 4.6ms preprocess, 92.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "['person', 'couch']\n",
      "\n",
      "0: 448x640 (no detections), 96.3ms\n",
      "Speed: 3.0ms preprocess, 96.3ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[]\n",
      "\n",
      "0: 480x640 1 person, 98.4ms\n",
      "Speed: 2.8ms preprocess, 98.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "['person']\n"
     ]
    }
   ],
   "source": [
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "i = 0\n",
    "for img_name in image_data:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "\n",
    "        # Load a model\n",
    "        model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "        img_name = img_name.replace(\"\\n\",\"\")\n",
    "\n",
    "        img= Image.open(image_directory + img_name)\n",
    "\n",
    "        results = model(img)  # predict on an image or image directory\n",
    "        names = model.names\n",
    "        #bounding boxes and confidence scores\n",
    "\n",
    "        for result in results:\n",
    "            detected_objects = []\n",
    "            detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "            result.save()\n",
    "            result.show()\n",
    "            # Display the detected objects\n",
    "            ids = result.boxes.cls\n",
    "            for id in ids:\n",
    "                name = names[int(id)]\n",
    "                if (name not in detected_objects):\n",
    "                    detected_objects.append(name)\n",
    "            print(detected_objects)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image {img_name} not found. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    i+=1\n",
    "    if (i == 7):\n",
    "        break\n",
    "   \n",
    "    detected_objects.clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad4d0a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 (no detections), 106.8ms\n",
      "Speed: 4.6ms preprocess, 106.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "[]\n",
      "else\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    " \n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "image_data = df[\"img_name\"].tolist()\n",
    " \n",
    "i = 0\n",
    "for img_name in image_data:\n",
    "    # Load a model\n",
    "    model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "    animals = ['bear', 'penguin', 'polar bear']\n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    " \n",
    "    image = Image.open(image_directory + img_name)\n",
    "    image.show()\n",
    "   \n",
    "    results = model(image)  # predict on an image or image directory\n",
    "    names = model.names\n",
    "    #bounding boxes and confidence scores\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "        result.save()\n",
    "        result.show()\n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "       \n",
    "        if ('person' in detected_objects):\n",
    "            #TODO FACE RECOGNITION\n",
    "            print('person')\n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "           \n",
    "           \n",
    "   \n",
    "    # long description model\n",
    "    model_id = \"vikhyatk/moondream2\"\n",
    "    revision = \"2024-04-02\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, trust_remote_code=True, revision=revision\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    " \n",
    "    enc_image = model.encode_image(image)\n",
    "    moonDreamResult = model.answer_question(enc_image, \"Describe this image.\", tokenizer)\n",
    "    print(moonDreamResult)\n",
    "   \n",
    "    # short description model\n",
    "    imgToText = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "    resultShortDescription = imgToText(image)\n",
    "   \n",
    "    ## compare those two models\n",
    "    print(resultShortDescription)\n",
    "\n",
    " \n",
    "    i+=1\n",
    "    if (i == 7):\n",
    "        break\n",
    "   \n",
    "    detected_objects.clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d72fd9",
   "metadata": {},
   "source": [
    "Valid pipeline plus csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade14420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run it later it takes so much time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "i = 0\n",
    "count = 0\n",
    "for img_name in image_data: \n",
    "    # Load a model\n",
    "    model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "    animals = ['bear', 'penguin', 'polar bear'] \n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    " \n",
    "    image = Image.open(image_directory + img_name)\n",
    "    image.show()\n",
    "   \n",
    "    results = model(image)  # predict on an image or image directory\n",
    "    names = model.names\n",
    "    #bounding boxes and confidence scores\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "        result.save()\n",
    "        result.show()\n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "        \n",
    "        if ('person' in detected_objects):\n",
    "            #TODO FACE RECOGNITION\n",
    "            print('person')\n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # long description model\n",
    "    model_id = \"vikhyatk/moondream2\"\n",
    "    revision = \"2024-04-02\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, trust_remote_code=True, revision=revision\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    " \n",
    "    enc_image = model.encode_image(image)\n",
    "    moonDreamResult = model.answer_question(enc_image, \"Describe this image.\", tokenizer)\n",
    "    print(moonDreamResult)\n",
    "    \n",
    "    # short description model\n",
    "    imgToText = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "    resultShortDescription = imgToText(image)\n",
    "    print(resultShortDescription)    \n",
    "    ## compare those two models #TODO\n",
    "    \n",
    "    \n",
    "    # Depth Anything for Depth Estimation - Foreground/Background\n",
    "    #pipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-large-hf\")\n",
    "    #depth = pipe(image)[\"depth\"]\n",
    "    #cv2.imshow('Depth Anything', depth)\n",
    "    \n",
    "    i+=1\n",
    "    if (i == 4):\n",
    "        break\n",
    "\n",
    "    \n",
    "    # image_name, object_detection_results, description_long, description_short, description_comparison, \n",
    "    num_entries = 4\n",
    "    with h5py.File('climatevisions_2019_popular_results.h5', 'w') as hf:\n",
    "        tweet_data_group = hf.create_group('tweet_data')\n",
    "        \n",
    "        image_name_dataset = tweet_data_group.create_dataset('image_name', shape=(5000,), dtype=h5py.string_dtype())\n",
    "        object_detection_results_dataset = tweet_data_group.create_dataset('object_detection_results', shape=(5000,), dtype=h5py.string_dtype())\n",
    "        description_long_dataset = tweet_data_group.create_dataset('description_long', shape=(5000,), dtype=h5py.string_dtype())\n",
    "        description_short_dataset = tweet_data_group.create_dataset('description_short', shape=(5000,), dtype=h5py.string_dtype())\n",
    "        description_comparison_dataset = tweet_data_group.create_dataset('description_comparison', shape=(5000,), dtype=h5py.string_dtype())\n",
    "        \n",
    "        object_string = ''\n",
    "        for index, object in enumerate(detected_objects):\n",
    "            if (index < len(detected_objects)):\n",
    "                object_string += object + ',' + ' '\n",
    "            else:\n",
    "                object_string += object\n",
    "        \n",
    "        hf[\"tweet_data/image_name\"][count] = img_name\n",
    "        hf[\"tweet_data/object_detection_results\"][count] = object_string\n",
    "        hf['tweet_data/description_long'][count] = moonDreamResult\n",
    "        \n",
    "        mapEntry = resultShortDescription[0]\n",
    "        firstValueShort = next(iter(mapEntry.values()))\n",
    "    \n",
    "        print(firstValueShort)\n",
    "        hf['tweet_data/description_short'][count] = firstValueShort\n",
    "        hf['tweet_data/description_comparison'][count] = ''\n",
    "        \n",
    "        \n",
    "    count += 1\n",
    "    detected_objects.clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c753c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write into .csv file\n",
    "#run it later\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from openpyxl import Workbook\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "image_data = df[\"img_name\"].tolist()\n",
    "\n",
    "header = [\"image_name\", \"object_detection_results\", \"description_short\", \"description_long\", \"description_comparison\"]\n",
    "ws.append(header)\n",
    "\n",
    "i = 0\n",
    "count = 0\n",
    "for img_name in image_data: \n",
    "    # Load a model\n",
    "    model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "    animals = ['bear', 'penguin', 'polar bear'] \n",
    "   \n",
    "    img_name = img_name.replace(\"\\n\",\"\")\n",
    " \n",
    "    image = Image.open(image_directory + img_name)\n",
    "   \n",
    "    results = model(image)  # predict on an image or image directory\n",
    "    names = model.names\n",
    "    #bounding boxes and confidence scores\n",
    " \n",
    "    for result in results:\n",
    "        detected_objects = []\n",
    "        detections = result.pred[0] if hasattr(result, 'pred') else None  # Get the detections if available\n",
    "        result.save()\n",
    "        result.show()\n",
    "        # Display the detected objects\n",
    "        ids = result.boxes.cls\n",
    "        for id in ids:\n",
    "            name = names[int(id)]\n",
    "            if (name not in detected_objects):\n",
    "                detected_objects.append(name)\n",
    "        print(detected_objects)\n",
    "        \n",
    "        if ('person' in detected_objects):\n",
    "            #TODO FACE RECOGNITION\n",
    "            print('person')\n",
    "        elif (any(animal in detected_objects for animal in animals)):\n",
    "            #TODO\n",
    "            print('animals')\n",
    "        else:\n",
    "            #TODO\n",
    "            print('else')\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # long description model\n",
    "    model_id = \"vikhyatk/moondream2\"\n",
    "    revision = \"2024-04-02\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, trust_remote_code=True, revision=revision\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n",
    " \n",
    "    enc_image = model.encode_image(image)\n",
    "    moonDreamResult = model.answer_question(enc_image, \"Describe this image.\", tokenizer)\n",
    "    print(moonDreamResult)\n",
    "    \n",
    "    # short description model\n",
    "    imgToText = pipeline(\"image-to-text\", model=\"aisak-ai/aisak-visual\")\n",
    "    resultShortDescription = imgToText(image)\n",
    "    print(resultShortDescription)    \n",
    "    ## compare those two models #TODO\n",
    "    \n",
    "    \n",
    "    # Depth Anything for Depth Estimation - Foreground/Background\n",
    "    #pipe = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-large-hf\")\n",
    "    #depth = pipe(image)[\"depth\"]\n",
    "    #cv2.imshow('Depth Anything', depth)\n",
    "    object_string = ''\n",
    "    for index, object in enumerate(detected_objects):\n",
    "        if (index < len(detected_objects)):\n",
    "            object_string += object + ',' + ' '\n",
    "        else:\n",
    "            object_string += object\n",
    "\n",
    "    i+=1\n",
    "    if (i == 5):\n",
    "        break\n",
    "    \n",
    "    mapEntry = resultShortDescription[0]\n",
    "    firstValueShort = next(iter(mapEntry.values()))\n",
    "\n",
    "    data_row = [img_name, object_string, moonDreamResult, firstValueShort, '']\n",
    "    ws.append(data_row)\n",
    "        \n",
    "    count += 1\n",
    "    detected_objects.clear\n",
    "\n",
    "\n",
    "wb.save(\"results.xlsx\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "af4749c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: fountain\n"
     ]
    }
   ],
   "source": [
    "#google/vit-base-patch16-224\n",
    "#model for image classification\n",
    "#works well for images with a single object, e.g. a cat, a dog,\n",
    "#animals: good, people: not good, objects: good\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1205801708644978689_2019-12-14.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a87e354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normal'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#detects if a image is normal or not safe for work\n",
    "# Load model directly\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageClassification, ViTImageProcessor\n",
    "\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1086663702567374848_2019-01-19.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\n",
    "processor = ViTImageProcessor.from_pretrained('Falconsai/nsfw_image_detection')\n",
    "with torch.no_grad():\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "model.config.id2label[predicted_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79bcf633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American egret, great white heron, Egretta albus\n"
     ]
    }
   ],
   "source": [
    "#model for image classification\n",
    "#works well with animals, objects, not so good with people\n",
    "#detects people as clothes\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1205801708644978689_2019-12-14.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8b108784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albatross, mollymawk\n"
     ]
    }
   ],
   "source": [
    "#model to classify images\n",
    "#works well, doesnt classify the image as a person, but as a clothes\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVision-1\\\\Dataset_small\\\\id_1205801708644978689_2019-12-14.jpg\"\n",
    "image = Image.open(image_path)\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1604a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sadness', 7: 'surprise'}\n",
      "Predicted label: sadness\n"
     ]
    }
   ],
   "source": [
    "#model for facial emotion recognition\n",
    "#works pretty well\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image_path = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\ProjectCode\\\\Team_Project_ComputerVisi    on-1\\\\Dataset_small\\\\id_1173661994668515329_2019-09-16.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Load the image processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(\"Rajaram1996/FacialEmoRecog\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"Rajaram1996/FacialEmoRecog\")\n",
    "print(model.config.id2label)\n",
    "# Preprocess the image using the image processor\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Get the predicted logits from the model\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the predicted label index\n",
    "predicted_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "# Get the predicted label name using the model's config\n",
    "predicted_label_name = model.config.id2label[predicted_label]\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Predicted label:\", predicted_label_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
